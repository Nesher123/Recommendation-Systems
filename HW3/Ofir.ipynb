{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations Systems\n",
    "## Homework 3: Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit your solution in the form of an Jupyter notebook file (with extension ipynb).   \n",
    "Images of graphs or tables should be submitted as PNG or JPG files.   \n",
    "The code used to answer the questions should be included, runnable and documented in the notebook.   \n",
    "Python 3.6 should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this homework is to let you understand the concept of  recommendations based on implicit data which is very common in real life, and learn how ‘Deep neural networks’ components can be used to implement a collaborative filtering and hybrid approach recommenders.  \n",
    "Implementation example is presented in the <a href='https://colab.research.google.com/drive/1v72_zpCObTFMbNnQXUknoQVXR1vBRX6_?usp=sharing'>NeuralCollaborativeFiltering_Implicit</a> notebook in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset based on the <a href='https://grouplens.org/datasets/movielens/1m/'>MovieLens 1M rating dataset</a> after some pre-processing to adapt it to an implicit feedback use case scenario.  \n",
    "You can download the dataset used by <a href='https://github.com/hexiangnan/neural_collaborative_filtering'>this implementation</a> of the paper Neural Collaborative Filtering or from the NeuralCollaborativeFiltering_implicit notebook in Moodle.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from keras.layers import Embedding, Input, Dense, Reshape,  Flatten, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras.layers import Multiply, Concatenate\n",
    "\n",
    "# from time import time\n",
    "# import multiprocessing as mp\n",
    "# import sys\n",
    "# import math\n",
    "# import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment-out on initial notebook run (we need some files fromt this repository)\n",
    "# !git clone https://github.com/hexiangnan/neural_collaborative_filtering.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "\n",
    "# Read the training file\n",
    "training = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.train.rating', sep='\\t', names=column_names)\n",
    "\n",
    "# Read the test file\n",
    "test_rating = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.test.rating', sep='\\t', names=column_names)\n",
    "\n",
    "\n",
    "negative_ids = ['(user_id, item_id)']\n",
    "\n",
    "for i in range(1,100):\n",
    "    negative_ids.append(f'id-{i}')\n",
    "\n",
    "test_negative = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.test.negative', sep='\\t', names=negative_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>978302268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id  rating  timestamp\n",
       "13        0        8       4  978302268"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.loc[(training['user_id'] == 0) & (training['item_id'] == 8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. This implementation contains one file for training and two files for testing:\n",
    "- ml-1m.train.rating\n",
    "- ml-1m.test.rating\n",
    "- ml-1m.test.negative\n",
    "\n",
    "**Explain** the role and structure of each file and how it was created from the original MovieLens 1M rating dataset.\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ml-1m.train.rating:\n",
    "- Training file\n",
    "- Each line is a training instance: userID\\t itemID\\t rating\\t timestamp (if exists)\n",
    "- 1 million ratings, where each user has at least 20 ratings\n",
    "- Similar to the training data from previous HWs\n",
    "\n",
    "ml-1m.test.rating:\n",
    "- Test file (positive instances)\n",
    "- Each line is a testing instance: userID\\t itemID\\t rating\\t timestamp (if exists)\n",
    "\n",
    "ml-1m.test.negative:\n",
    "- Test file (negative instances)\n",
    "- Each line corresponds to the line of test.rating, containing 99 negative samples\n",
    "- Each line is in the format: (userID,itemID)\\t negativeItemID1\\t negativeItemID2...\n",
    "- This is the set of instances the user didn't interact with (rated)\n",
    "- Only the first cell (userID,itemID) is the user and an item s/he DID interact with\n",
    "- itemID is the most recent item the user interacted with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. **Explain** how the training dataset is created.\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data (only the positive ratings, none of the negative ones).\n",
    "We go over the train data and read every tuple of user & item.\n",
    "\n",
    "Firse, we add the userID to the trainining dataset with a label '1'.\n",
    "\n",
    "We randomly choose 4 negative items (the \"num_negatives\" parameter. Can be a differernt number) and add it to the training dataset.\n",
    "Since the matrix is vary sparse, there is a high probability to randomly choose an index of a negative item.\n",
    "Then we add it to the trainining dataset with a label '0'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. **Explain** how the test dataset is created.\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to the way the training dataset is constructed, but with data the training dataset this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 2: Neural Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negatives = 4\n",
    "TOP_K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "M7nlDtDaCakQ"
   },
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [0]*((num_negatives + 1)*len(train)),[0]*((num_negatives + 1)*len(train)),[1]\n",
    "    num_users = train.shape[0]\n",
    "    \n",
    "    negatives = [0]*num_negatives\n",
    "    labels.extend(negatives)\n",
    "    total_labels = []\n",
    "    list(map(lambda x: total_labels.extend(labels), range(len(train))))\n",
    "    ic(len(total_labels), total_labels[:20])\n",
    "#     return\n",
    "    percent_1 = int(len(train)/1000)\n",
    "    ic(percent_1)\n",
    "    for idx_i in range(len(train)):\n",
    "        curr_index = idx_i * (num_negatives + 1)\n",
    "        if idx_i != 0 and idx_i % percent_1 == 0:\n",
    "            print(f'{int(idx_i/percent_1)}%')\n",
    "        u = train.iloc[idx_i].user_id\n",
    "        i = train.iloc[idx_i].item_id\n",
    "#         ic(u,i)\n",
    "        user_input[curr_index:curr_index + (num_negatives + 1)] = [u]*(num_negatives + 1)\n",
    "#         user_input.extend([u]*(num_negatives + 1))\n",
    "        item_input[curr_index] = i\n",
    "\n",
    "        items = training[training['user_id'] == u].item_id.to_numpy()\n",
    "        sample_items = items[np.random.choice(len(items), size=num_negatives, replace=False)]\n",
    "        item_input[curr_index+1:curr_index + (num_negatives + 1)] = sample_items\n",
    "\n",
    "    return user_input, item_input, total_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(total_labels): 4970845\n",
      "    total_labels[:20]: [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "ic| percent_1: 994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1%\n",
      "2%\n",
      "3%\n",
      "4%\n",
      "5%\n",
      "6%\n",
      "7%\n",
      "8%\n",
      "9%\n",
      "10%\n",
      "11%\n",
      "12%\n",
      "13%\n",
      "14%\n",
      "15%\n",
      "16%\n",
      "17%\n",
      "18%\n",
      "19%\n",
      "20%\n",
      "21%\n",
      "22%\n",
      "23%\n",
      "24%\n",
      "25%\n",
      "26%\n",
      "27%\n",
      "28%\n",
      "29%\n",
      "30%\n",
      "31%\n",
      "32%\n",
      "33%\n",
      "34%\n",
      "35%\n",
      "36%\n",
      "37%\n",
      "38%\n",
      "39%\n",
      "40%\n",
      "41%\n",
      "42%\n",
      "43%\n",
      "44%\n",
      "45%\n",
      "46%\n",
      "47%\n",
      "48%\n",
      "49%\n",
      "50%\n",
      "51%\n",
      "52%\n",
      "53%\n",
      "54%\n",
      "55%\n",
      "56%\n",
      "57%\n",
      "58%\n",
      "59%\n",
      "60%\n",
      "61%\n",
      "62%\n",
      "63%\n",
      "64%\n",
      "65%\n",
      "66%\n",
      "67%\n",
      "68%\n",
      "69%\n",
      "70%\n",
      "71%\n",
      "72%\n",
      "73%\n",
      "74%\n",
      "75%\n",
      "76%\n",
      "77%\n",
      "78%\n",
      "79%\n",
      "80%\n",
      "81%\n",
      "82%\n",
      "83%\n",
      "84%\n",
      "85%\n",
      "86%\n",
      "87%\n",
      "88%\n",
      "89%\n",
      "90%\n",
      "91%\n",
      "92%\n",
      "93%\n",
      "94%\n",
      "95%\n",
      "96%\n",
      "97%\n",
      "98%\n",
      "99%\n",
      "100%\n",
      "101%\n",
      "102%\n",
      "103%\n",
      "104%\n",
      "105%\n",
      "106%\n",
      "107%\n",
      "108%\n",
      "109%\n",
      "110%\n",
      "111%\n",
      "112%\n",
      "113%\n",
      "114%\n",
      "115%\n",
      "116%\n",
      "117%\n",
      "118%\n",
      "119%\n",
      "120%\n",
      "121%\n",
      "122%\n",
      "123%\n",
      "124%\n",
      "125%\n",
      "126%\n",
      "127%\n",
      "128%\n",
      "129%\n",
      "130%\n",
      "131%\n",
      "132%\n",
      "133%\n",
      "134%\n",
      "135%\n",
      "136%\n",
      "137%\n",
      "138%\n",
      "139%\n",
      "140%\n",
      "141%\n",
      "142%\n",
      "143%\n",
      "144%\n",
      "145%\n",
      "146%\n",
      "147%\n",
      "148%\n",
      "149%\n",
      "150%\n",
      "151%\n",
      "152%\n",
      "153%\n",
      "154%\n",
      "155%\n",
      "156%\n",
      "157%\n",
      "158%\n",
      "159%\n",
      "160%\n",
      "161%\n",
      "162%\n",
      "163%\n",
      "164%\n",
      "165%\n",
      "166%\n",
      "167%\n",
      "168%\n",
      "169%\n",
      "170%\n",
      "171%\n",
      "172%\n",
      "173%\n",
      "174%\n",
      "175%\n",
      "176%\n",
      "177%\n",
      "178%\n",
      "179%\n",
      "180%\n",
      "181%\n",
      "182%\n",
      "183%\n",
      "184%\n",
      "185%\n",
      "186%\n",
      "187%\n",
      "188%\n",
      "189%\n",
      "190%\n",
      "191%\n",
      "192%\n",
      "193%\n",
      "194%\n",
      "195%\n",
      "196%\n",
      "197%\n",
      "198%\n",
      "199%\n",
      "200%\n",
      "201%\n",
      "202%\n",
      "203%\n",
      "204%\n",
      "205%\n",
      "206%\n",
      "207%\n",
      "208%\n",
      "209%\n",
      "210%\n",
      "211%\n",
      "212%\n",
      "213%\n",
      "214%\n",
      "215%\n",
      "216%\n",
      "217%\n",
      "218%\n",
      "219%\n",
      "220%\n",
      "221%\n",
      "222%\n",
      "223%\n",
      "224%\n",
      "225%\n",
      "226%\n",
      "227%\n",
      "228%\n",
      "229%\n",
      "230%\n",
      "231%\n",
      "232%\n",
      "233%\n",
      "234%\n",
      "235%\n",
      "236%\n",
      "237%\n",
      "238%\n",
      "239%\n",
      "240%\n",
      "241%\n",
      "242%\n",
      "243%\n",
      "244%\n",
      "245%\n",
      "246%\n",
      "247%\n",
      "248%\n",
      "249%\n",
      "250%\n",
      "251%\n",
      "252%\n",
      "253%\n",
      "254%\n",
      "255%\n",
      "256%\n",
      "257%\n",
      "258%\n",
      "259%\n",
      "260%\n",
      "261%\n",
      "262%\n",
      "263%\n",
      "264%\n",
      "265%\n",
      "266%\n",
      "267%\n",
      "268%\n",
      "269%\n",
      "270%\n",
      "271%\n",
      "272%\n",
      "273%\n",
      "274%\n",
      "275%\n",
      "276%\n",
      "277%\n",
      "278%\n",
      "279%\n",
      "280%\n",
      "281%\n",
      "282%\n",
      "283%\n",
      "284%\n",
      "285%\n",
      "286%\n",
      "287%\n",
      "288%\n",
      "289%\n",
      "290%\n",
      "291%\n",
      "292%\n",
      "293%\n",
      "294%\n",
      "295%\n",
      "296%\n",
      "297%\n",
      "298%\n",
      "299%\n",
      "300%\n",
      "301%\n",
      "302%\n",
      "303%\n",
      "304%\n",
      "305%\n",
      "306%\n",
      "307%\n",
      "308%\n",
      "309%\n",
      "310%\n",
      "311%\n",
      "312%\n",
      "313%\n",
      "314%\n",
      "315%\n",
      "316%\n",
      "317%\n",
      "318%\n",
      "319%\n",
      "320%\n",
      "321%\n",
      "322%\n",
      "323%\n",
      "324%\n",
      "325%\n",
      "326%\n",
      "327%\n",
      "328%\n",
      "329%\n",
      "330%\n",
      "331%\n",
      "332%\n",
      "333%\n",
      "334%\n",
      "335%\n",
      "336%\n",
      "337%\n",
      "338%\n",
      "339%\n",
      "340%\n",
      "341%\n",
      "342%\n",
      "343%\n",
      "344%\n",
      "345%\n",
      "346%\n",
      "347%\n",
      "348%\n",
      "349%\n",
      "350%\n",
      "351%\n",
      "352%\n",
      "353%\n",
      "354%\n",
      "355%\n",
      "356%\n",
      "357%\n",
      "358%\n",
      "359%\n",
      "360%\n",
      "361%\n",
      "362%\n",
      "363%\n",
      "364%\n",
      "365%\n",
      "366%\n",
      "367%\n",
      "368%\n",
      "369%\n",
      "370%\n",
      "371%\n",
      "372%\n",
      "373%\n",
      "374%\n",
      "375%\n",
      "376%\n",
      "377%\n",
      "378%\n",
      "379%\n",
      "380%\n",
      "381%\n",
      "382%\n",
      "383%\n",
      "384%\n",
      "385%\n",
      "386%\n",
      "387%\n",
      "388%\n",
      "389%\n",
      "390%\n",
      "391%\n",
      "392%\n",
      "393%\n",
      "394%\n",
      "395%\n",
      "396%\n",
      "397%\n",
      "398%\n",
      "399%\n",
      "400%\n",
      "401%\n",
      "402%\n",
      "403%\n",
      "404%\n",
      "405%\n",
      "406%\n",
      "407%\n",
      "408%\n",
      "409%\n",
      "410%\n",
      "411%\n",
      "412%\n",
      "413%\n",
      "414%\n",
      "415%\n",
      "416%\n",
      "417%\n",
      "418%\n",
      "419%\n",
      "420%\n",
      "421%\n",
      "422%\n",
      "423%\n",
      "424%\n",
      "425%\n",
      "426%\n",
      "427%\n",
      "428%\n",
      "429%\n",
      "430%\n",
      "431%\n",
      "432%\n",
      "433%\n",
      "434%\n",
      "435%\n",
      "436%\n",
      "437%\n",
      "438%\n",
      "439%\n",
      "440%\n",
      "441%\n",
      "442%\n",
      "443%\n",
      "444%\n",
      "445%\n",
      "446%\n",
      "447%\n",
      "448%\n",
      "449%\n",
      "450%\n",
      "451%\n",
      "452%\n",
      "453%\n",
      "454%\n",
      "455%\n",
      "456%\n",
      "457%\n",
      "458%\n",
      "459%\n",
      "460%\n",
      "461%\n",
      "462%\n",
      "463%\n",
      "464%\n",
      "465%\n",
      "466%\n",
      "467%\n",
      "468%\n",
      "469%\n",
      "470%\n",
      "471%\n",
      "472%\n",
      "473%\n",
      "474%\n",
      "475%\n",
      "476%\n",
      "477%\n",
      "478%\n",
      "479%\n",
      "480%\n",
      "481%\n",
      "482%\n",
      "483%\n",
      "484%\n",
      "485%\n",
      "486%\n",
      "487%\n",
      "488%\n",
      "489%\n",
      "490%\n",
      "491%\n",
      "492%\n",
      "493%\n",
      "494%\n",
      "495%\n",
      "496%\n",
      "497%\n",
      "498%\n",
      "499%\n",
      "500%\n",
      "501%\n",
      "502%\n",
      "503%\n",
      "504%\n",
      "505%\n",
      "506%\n",
      "507%\n",
      "508%\n",
      "509%\n",
      "510%\n",
      "511%\n",
      "512%\n",
      "513%\n",
      "514%\n",
      "515%\n",
      "516%\n",
      "517%\n",
      "518%\n",
      "519%\n",
      "520%\n",
      "521%\n",
      "522%\n",
      "523%\n",
      "524%\n",
      "525%\n",
      "526%\n",
      "527%\n",
      "528%\n",
      "529%\n",
      "530%\n",
      "531%\n",
      "532%\n",
      "533%\n",
      "534%\n",
      "535%\n",
      "536%\n",
      "537%\n",
      "538%\n",
      "539%\n",
      "540%\n",
      "541%\n",
      "542%\n",
      "543%\n",
      "544%\n",
      "545%\n",
      "546%\n",
      "547%\n",
      "548%\n",
      "549%\n",
      "550%\n",
      "551%\n",
      "552%\n",
      "553%\n",
      "554%\n",
      "555%\n",
      "556%\n",
      "557%\n",
      "558%\n",
      "559%\n",
      "560%\n",
      "561%\n",
      "562%\n",
      "563%\n",
      "564%\n",
      "565%\n",
      "566%\n",
      "567%\n",
      "568%\n",
      "569%\n",
      "570%\n",
      "571%\n",
      "572%\n",
      "573%\n",
      "574%\n",
      "575%\n",
      "576%\n",
      "577%\n",
      "578%\n",
      "579%\n",
      "580%\n",
      "581%\n",
      "582%\n",
      "583%\n",
      "584%\n",
      "585%\n",
      "586%\n",
      "587%\n",
      "588%\n",
      "589%\n",
      "590%\n",
      "591%\n",
      "592%\n",
      "593%\n",
      "594%\n",
      "595%\n",
      "596%\n",
      "597%\n",
      "598%\n",
      "599%\n",
      "600%\n",
      "601%\n",
      "602%\n",
      "603%\n",
      "604%\n",
      "605%\n",
      "606%\n",
      "607%\n",
      "608%\n",
      "609%\n",
      "610%\n",
      "611%\n",
      "612%\n",
      "613%\n",
      "614%\n",
      "615%\n",
      "616%\n",
      "617%\n",
      "618%\n",
      "619%\n",
      "620%\n",
      "621%\n",
      "622%\n",
      "623%\n",
      "624%\n",
      "625%\n",
      "626%\n",
      "627%\n",
      "628%\n",
      "629%\n",
      "630%\n",
      "631%\n",
      "632%\n",
      "633%\n",
      "634%\n",
      "635%\n",
      "636%\n",
      "637%\n",
      "638%\n",
      "639%\n",
      "640%\n",
      "641%\n",
      "642%\n",
      "643%\n",
      "644%\n",
      "645%\n",
      "646%\n",
      "647%\n",
      "648%\n",
      "649%\n",
      "650%\n",
      "651%\n",
      "652%\n",
      "653%\n",
      "654%\n",
      "655%\n",
      "656%\n",
      "657%\n",
      "658%\n",
      "659%\n",
      "660%\n",
      "661%\n",
      "662%\n",
      "663%\n",
      "664%\n",
      "665%\n",
      "666%\n",
      "667%\n",
      "668%\n",
      "669%\n",
      "670%\n",
      "671%\n",
      "672%\n",
      "673%\n",
      "674%\n",
      "675%\n",
      "676%\n",
      "677%\n",
      "678%\n",
      "679%\n",
      "680%\n",
      "681%\n",
      "682%\n",
      "683%\n",
      "684%\n",
      "685%\n",
      "686%\n",
      "687%\n",
      "688%\n",
      "689%\n",
      "690%\n",
      "691%\n",
      "692%\n",
      "693%\n",
      "694%\n",
      "695%\n",
      "696%\n",
      "697%\n",
      "698%\n",
      "699%\n",
      "700%\n",
      "701%\n",
      "702%\n",
      "703%\n",
      "704%\n",
      "705%\n",
      "706%\n",
      "707%\n",
      "708%\n",
      "709%\n",
      "710%\n",
      "711%\n",
      "712%\n",
      "713%\n",
      "714%\n",
      "715%\n",
      "716%\n",
      "717%\n",
      "718%\n",
      "719%\n",
      "720%\n",
      "721%\n",
      "722%\n",
      "723%\n",
      "724%\n",
      "725%\n",
      "726%\n",
      "727%\n",
      "728%\n",
      "729%\n",
      "730%\n",
      "731%\n",
      "732%\n",
      "733%\n",
      "734%\n",
      "735%\n",
      "736%\n",
      "737%\n",
      "738%\n",
      "739%\n",
      "740%\n",
      "741%\n",
      "742%\n",
      "743%\n",
      "744%\n",
      "745%\n",
      "746%\n",
      "747%\n",
      "748%\n",
      "749%\n",
      "750%\n",
      "751%\n",
      "752%\n",
      "753%\n",
      "754%\n",
      "755%\n",
      "756%\n",
      "757%\n",
      "758%\n",
      "759%\n",
      "760%\n",
      "761%\n",
      "762%\n",
      "763%\n",
      "764%\n",
      "765%\n",
      "766%\n",
      "767%\n",
      "768%\n",
      "769%\n",
      "770%\n",
      "771%\n",
      "772%\n",
      "773%\n",
      "774%\n",
      "775%\n",
      "776%\n",
      "777%\n",
      "778%\n",
      "779%\n",
      "780%\n",
      "781%\n",
      "782%\n",
      "783%\n",
      "784%\n",
      "785%\n",
      "786%\n",
      "787%\n",
      "788%\n",
      "789%\n",
      "790%\n",
      "791%\n",
      "792%\n",
      "793%\n",
      "794%\n",
      "795%\n",
      "796%\n",
      "797%\n",
      "798%\n",
      "799%\n",
      "800%\n",
      "801%\n",
      "802%\n",
      "803%\n",
      "804%\n",
      "805%\n",
      "806%\n",
      "807%\n",
      "808%\n",
      "809%\n",
      "810%\n",
      "811%\n",
      "812%\n",
      "813%\n",
      "814%\n",
      "815%\n",
      "816%\n",
      "817%\n",
      "818%\n",
      "819%\n",
      "820%\n",
      "821%\n",
      "822%\n",
      "823%\n",
      "824%\n",
      "825%\n",
      "826%\n",
      "827%\n",
      "828%\n",
      "829%\n",
      "830%\n",
      "831%\n",
      "832%\n",
      "833%\n",
      "834%\n",
      "835%\n",
      "836%\n",
      "837%\n",
      "838%\n",
      "839%\n",
      "840%\n",
      "841%\n",
      "842%\n",
      "843%\n",
      "844%\n",
      "845%\n",
      "846%\n",
      "847%\n",
      "848%\n",
      "849%\n",
      "850%\n",
      "851%\n",
      "852%\n",
      "853%\n",
      "854%\n",
      "855%\n",
      "856%\n",
      "857%\n",
      "858%\n",
      "859%\n",
      "860%\n",
      "861%\n",
      "862%\n",
      "863%\n",
      "864%\n",
      "865%\n",
      "866%\n",
      "867%\n",
      "868%\n",
      "869%\n",
      "870%\n",
      "871%\n",
      "872%\n",
      "873%\n",
      "874%\n",
      "875%\n",
      "876%\n",
      "877%\n",
      "878%\n",
      "879%\n",
      "880%\n",
      "881%\n",
      "882%\n",
      "883%\n",
      "884%\n",
      "885%\n",
      "886%\n",
      "887%\n",
      "888%\n",
      "889%\n",
      "890%\n",
      "891%\n",
      "892%\n",
      "893%\n",
      "894%\n",
      "895%\n",
      "896%\n",
      "897%\n",
      "898%\n",
      "899%\n",
      "900%\n",
      "901%\n",
      "902%\n",
      "903%\n",
      "904%\n",
      "905%\n",
      "906%\n",
      "907%\n",
      "908%\n",
      "909%\n",
      "910%\n",
      "911%\n",
      "912%\n",
      "913%\n",
      "914%\n",
      "915%\n",
      "916%\n",
      "917%\n",
      "918%\n",
      "919%\n",
      "920%\n",
      "921%\n",
      "922%\n",
      "923%\n",
      "924%\n",
      "925%\n",
      "926%\n",
      "927%\n",
      "928%\n",
      "929%\n",
      "930%\n",
      "931%\n",
      "932%\n",
      "933%\n",
      "934%\n",
      "935%\n",
      "936%\n",
      "937%\n",
      "938%\n",
      "939%\n",
      "940%\n",
      "941%\n",
      "942%\n",
      "943%\n",
      "944%\n",
      "945%\n",
      "946%\n",
      "947%\n",
      "948%\n",
      "949%\n",
      "950%\n",
      "951%\n",
      "952%\n",
      "953%\n",
      "954%\n",
      "955%\n",
      "956%\n",
      "957%\n",
      "958%\n",
      "959%\n",
      "960%\n",
      "961%\n",
      "962%\n",
      "963%\n",
      "964%\n",
      "965%\n",
      "966%\n",
      "967%\n",
      "968%\n",
      "969%\n",
      "970%\n",
      "971%\n",
      "972%\n",
      "973%\n",
      "974%\n",
      "975%\n",
      "976%\n",
      "977%\n",
      "978%\n",
      "979%\n",
      "980%\n",
      "981%\n",
      "982%\n",
      "983%\n",
      "984%\n",
      "985%\n",
      "986%\n",
      "987%\n",
      "988%\n",
      "989%\n",
      "990%\n",
      "991%\n",
      "992%\n",
      "993%\n",
      "994%\n",
      "995%\n",
      "996%\n",
      "997%\n",
      "998%\n",
      "999%\n",
      "1000%\n"
     ]
    }
   ],
   "source": [
    "training_data = get_train_instances(training, num_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('training_data.pkl', 'wb') as f:\n",
    "    pickle.dump(training_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = train_instances\n",
    "user_input, item_input, total_labels = training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>item_input</th>\n",
       "      <th>total_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_input  item_input  total_labels\n",
       "0           0          32             1\n",
       "1           0          11             0\n",
       "2           0          27             0\n",
       "3           0          36             0\n",
       "4           0          33             0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(user_input, item_input, total_labels)), \n",
    "               columns =['user_input', 'item_input', 'total_labels'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('training_data_latest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Build the following four models using the neural collaborative filtering approach: \n",
    "- Matrix Factorization (MF)\n",
    "- Multi layer perceptron (MLP)\n",
    "- Generalized Matrix Factorization (GMF) \n",
    "- NeuroMatrixFactorization (NMF)\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MF_model(num_users, num_items, latent_dim):\n",
    "    '''Vanilla Matrix Factorization'''\n",
    "    \n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding', input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding', input_length=1)   \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings\n",
    "    #prediction = merge([user_latent, item_latent], mode = 'dot')\n",
    "    prediction = keras.layers.dot([user_latent,item_latent], axes=1,normalize=False)\n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_GMF_model(num_users, num_items, latent_dim, regs=None, activation='sigmoid'):\n",
    "    '''Generalized Matrix Factorization'''\n",
    "        \n",
    "    if not regs:\n",
    "        regs = [[0,0]]\n",
    "    \n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01))  #init = init_normal, \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings \n",
    "    predict_vector = Multiply()([user_latent, item_latent]) #merge([user_latent, item_latent], mode = 'mul')\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation=activation, kernel_initializer='lecun_uniform', name = 'prediction')(predict_vector)\n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_MLP_model(num_users, num_items, latent_dim, regs=None, layers = None, activation='sigmoid'):\n",
    "    '''Multi-Layer Perceptron'''\n",
    "    \n",
    "    if not regs:\n",
    "        regs = [[0,0],0,0]\n",
    "    \n",
    "    if not layers:\n",
    "        layers = [20,10]\n",
    "    \n",
    "    assert len(layers) + 1 == len(regs), 'the number of regs is equal to number of layers + the embedding layer'\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    \n",
    "    # Concatenation of embedding layers\n",
    "    vector = Concatenate(axis=-1)([user_latent, item_latent])#merge([user_latent, item_latent], mode = 'concat')\n",
    "    \n",
    "    # MLP layers\n",
    "    for idx in range(num_layer):\n",
    "        layer = Dense(layers[idx], kernel_regularizer = l2(regs[idx+1]), activation='relu', name = 'layer%d' %idx)\n",
    "        vector = layer(vector)\n",
    "        \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation=activation, kernel_initializer='lecun_uniform', name = 'prediction')(vector)\n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_NMF_model(num_users, num_items, latent_dim_GMF, latent_dim_MLP, reg_GMF=None, regs_MLP=None, layers=None, activation='sigmoid'):\n",
    "    '''Neural matrix factorization'''\n",
    "    \n",
    "    if not reg_GMF:\n",
    "        reg_GMF=[[0,0]]\n",
    "        \n",
    "    if not regs_MLP:\n",
    "        regs_MLP=[[0,0],0,0]\n",
    "        \n",
    "    if not laters:\n",
    "        layers=[20,10]\n",
    "    \n",
    "    assert len(layers) + 1 == len(regs_MLP), 'the number of regs is equal to number of layers + the embedding layer'\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    # Embedding layer\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim_GMF, name = 'MF_user_embedding',\n",
    "                                   embeddings_regularizer = l2(reg_GMF[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim_GMF, name = 'MF_item_embedding',\n",
    "                                   embeddings_regularizer = l2(reg_GMF[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01))  #init = init_normal, \n",
    "    \n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim_MLP, name = 'MLP_user_embedding',\n",
    "                                   embeddings_regularizer = l2(regs_MLP[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim_MLP, name = 'MLP_item_embedding',\n",
    "                                   embeddings_regularizer = l2(regs_MLP[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    \n",
    "    # MF part\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    mf_vector = Multiply()([mf_user_latent, mf_item_latent]) #merge([mf_user_latent, mf_item_latent], mode = 'mul') # element-wise multiply\n",
    "\n",
    "    # MLP part\n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    mlp_vector = Concatenate(axis=-1)([mlp_user_latent, mlp_item_latent])#merge([mlp_user_latent, mlp_item_latent], mode = 'concat')\n",
    "    \n",
    "    for idx in range(num_layer):\n",
    "        layer =  Dense(layers[idx], kernel_regularizer = l2(regs_MLP[idx+1]), activation='tanh', name = 'layer%d' %idx)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    # Concatenate MF and MLP parts\n",
    "    predict_vector = Concatenate(axis=-1)([mf_vector, mlp_vector])\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation=activation, kernel_initializer='lecun_uniform', name = \"prediction\")(predict_vector)    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factors = 8 # size of embedding size. Can be split to 4 different params potentially.\n",
    "num_negatives = 4 # how many negative samples per positive sample?\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "verbose = 1\n",
    "write_model=False\n",
    "topK = 10 # used to evaluate the model. Top K recommendations are used.\n",
    "evaluation_threads = 1 \n",
    "model_out_file = 'Pretrain/%s_GMF_%d_%d.h5' %(dataset, num_factors, time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models\n",
    "mlp_model = get_MLP_model(num_users, num_items, num_factors, regs = [[0,0],0,0,0], layers = [32,16,8])\n",
    "gmf_model = get_GMF_model(num_users, num_items, num_factors, regs = [[0,0]])\n",
    "nmf_model = get_NMF_model(num_users, num_items, latent_dim_GMF=num_factors, latent_dim_MLP=num_factors, reg_GMF=[[0,0]], regs_MLP=[[0,0],0,0,0], layers=[32,16,8])\n",
    "\n",
    "mlp_model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
    "gmf_model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
    "nmf_model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Train and evaluate the recommendations accuracy of three models: \n",
    "- MF or GMF\n",
    "- MLP\n",
    "- NMF\n",
    "\n",
    "Compare the learning curve and recommendations accuracy using NDCG and MRR metrics with cutoff values of 5 and 10.   \n",
    "Discuss the comparison.\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phL8meRGnql2"
   },
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq # for retrieval topK\n",
    "import multiprocessing\n",
    "from time import time\n",
    "#from numba import jit, autojit\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_ratings, test_negatives, K):\n",
    "    \"\"\"\n",
    "    Evaluate the performance (MRR, NDCG) of top-K recommendation\n",
    "    Return: score of each test rating.\n",
    "    \"\"\"\n",
    "    mrrs, ndcgs = zip(*[eval_one_rating(model, test_ratings, test_negatives, idx, K) for idx in range(len(test_ratings))])\n",
    "    return np.array(mrrs).mean(), np.array(ndcgs).mean()\n",
    "\n",
    "\n",
    "def eval_one_rating(model, test_ratings, test_negatives, idx, K):\n",
    "    rating = test_ratings[idx]\n",
    "    items = test_negatives[idx]\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    items.append(gtItem)\n",
    "    # Get prediction scores\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype = 'int32')\n",
    "    predictions = model.predict([users, np.array(items)], \n",
    "                                 batch_size=100, verbose=0)\n",
    "    \n",
    "    for i in range(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = predictions[i]\n",
    "    \n",
    "    items.pop()\n",
    "    \n",
    "    # Evaluate top rank list\n",
    "    ranklist = heapq.nlargest(_K, map_item_score, key=map_item_score.get)\n",
    "    mrr = getMRR(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    \n",
    "    return mrr, ndcg\n",
    "\n",
    "\n",
    "def getMRR(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        \n",
    "        if item == gtItem:\n",
    "            return 1/(i+1)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        \n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JfUQa6wDf_21",
    "outputId": "f470834c-b8c3-47b2-d2bc-f4ea34f56d0e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gmf_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "In  \u001b[0;34m[2]\u001b[0m:\nLine \u001b[0;34m1\u001b[0m:     models = [(\u001b[33m'\u001b[39;49;00m\u001b[33mGMF\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, gmf_model), (\u001b[33m'\u001b[39;49;00m\u001b[33mMLP\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, mlp_model), (\u001b[33m'\u001b[39;49;00m\u001b[33mNMF\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, nmf_model)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gmf_model' is not defined\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "models = [('GMF', gmf_model), ('MLP', mlp_model), ('NMF', nmf_model)]\n",
    "\n",
    "def initPerformance():\n",
    "  # Init performance\n",
    "  for name, model in models:\n",
    "    t1 = time()\n",
    "    mrr, ndcg = evaluate_model(model, test_rating, test_negative, TOP_K)\n",
    "    print(f'{name} Init: MRR = {mrr:.4f}, NDCG = {ndcg:.4f}\\t time = [{(time()-t1)/60}s]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. How the values of MRR and NDCG are differ from the results you got in the previous exercises which implemented the explicit recommendation approach. \n",
    "What are the difference in preparing the dataset for evaluation.\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. How will you measure item similarity using the NeuMF model?\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 3: Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. One of the enhancements presented in the Neural Collaborative Filtering paper is the usage of probabilistic activation function (the sigmoid) and binary cross entropy loss function.   \n",
    "\n",
    "Select one of the models you implemented in question 2 and change the loss function to a Mean Squared Error and the activation function of the last layer to RELU.   \n",
    "\n",
    "Train the model and evaluate it in a similar way to what you did in question 2. \n",
    "Compare the results and discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-1ff5a7b22abc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model_3a = get_MLP_model(num_users, num_items, latent_dim_GMF=num_factors, latent_dim_MLP=num_factors, \n\u001b[0m\u001b[0;32m      2\u001b[0m                          \u001b[0mreg_GMF\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregs_MLP\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                          activation='relu')\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_3a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_users' is not defined"
     ]
    }
   ],
   "source": [
    "model_3a = get_MLP_model(num_users, num_items, num_factors, regs = [[0,0],0,0,0], layers = [32,16,8], activation='relu')\n",
    "model_3a.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "print(model_3a.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
