{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations Systems\n",
    "## Homework 3: Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit your solution in the form of an Jupyter notebook file (with extension ipynb).   \n",
    "Images of graphs or tables should be submitted as PNG or JPG files.   \n",
    "The code used to answer the questions should be included, runnable and documented in the notebook.   \n",
    "Python 3.6 should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this homework is to let you understand the concept of  recommendations based on implicit data which is very common in real life, and learn how ‘Deep neural networks’ components can be used to implement a collaborative filtering and hybrid approach recommenders.  \n",
    "Implementation example is presented in the <a href='https://colab.research.google.com/drive/1v72_zpCObTFMbNnQXUknoQVXR1vBRX6_?usp=sharing'>NeuralCollaborativeFiltering_Implicit</a> notebook in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset based on the <a href='https://grouplens.org/datasets/movielens/1m/'>MovieLens 1M rating dataset</a> after some pre-processing to adapt it to an implicit feedback use case scenario.  \n",
    "You can download the dataset used by <a href='https://github.com/hexiangnan/neural_collaborative_filtering'>this implementation</a> of the paper Neural Collaborative Filtering or from the NeuralCollaborativeFiltering_implicit notebook in Moodle.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from keras.layers import Embedding, Input, Dense, Reshape,  Flatten, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras.layers import Multiply, Concatenate\n",
    "\n",
    "# from time import time\n",
    "# import multiprocessing as mp\n",
    "# import sys\n",
    "# import math\n",
    "# import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/hexiangnan/neural_collaborative_filtering.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "\n",
    "# Read the training file\n",
    "training = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.train.rating', sep='\\t', names=column_names)\n",
    "\n",
    "# Read the test file\n",
    "test_rating = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.test.rating', sep='\\t', names=column_names)\n",
    "\n",
    "\n",
    "negative_ids = ['(user_id, item_id)']\n",
    "\n",
    "for i in range(1,100):\n",
    "    negative_ids.append(f'id-{i}')\n",
    "\n",
    "test_negative = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.test.negative', sep='\\t', names=negative_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>978302268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id  rating  timestamp\n",
       "13        0        8       4  978302268"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.loc[(training['user_id'] == 0) & (training['item_id'] == 8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. This implementation contains one file for training and two files for testing:\n",
    "- ml-1m.train.rating\n",
    "- ml-1m.test.rating\n",
    "- ml-1m.test.negative\n",
    "\n",
    "**Explain** the role and structure of each file and how it was created from the original MovieLens 1M rating dataset.\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ml-1m.train.rating:\n",
    "- Training file\n",
    "- Each line is a training instance: userID\\t itemID\\t rating\\t timestamp (if exists)\n",
    "- 1 million ratings, where each user has at least 20 ratings\n",
    "- Similar to the training data from previous HWs\n",
    "\n",
    "ml-1m.test.rating:\n",
    "- Test file (positive instances)\n",
    "- Each line is a testing instance: userID\\t itemID\\t rating\\t timestamp (if exists)\n",
    "\n",
    "ml-1m.test.negative:\n",
    "- Test file (negative instances)\n",
    "- Each line corresponds to the line of test.rating, containing 99 negative samples\n",
    "- Each line is in the format: (userID,itemID)\\t negativeItemID1\\t negativeItemID2..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. **Explain** how the training dataset is created.\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. **Explain** how the test dataset is created.\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 2: Neural Collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "M7nlDtDaCakQ"
   },
   "outputs": [],
   "source": [
    "########## Maybe need to change like the \"maybe\" file\n",
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    \n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        \n",
    "        # negative instances\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            \n",
    "            while (u, j) in train:\n",
    "                j = np.random.randint(num_items)\n",
    "                \n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    \n",
    "    return user_input, item_input, labelsem_input, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Build the following four models using the neural collaborative filtering approach: \n",
    "- Matrix Factorization (MF)\n",
    "- Multi layer perceptron (MLP)\n",
    "- Generalized Matrix Factorization (GMF) \n",
    "- NeuroMatrixFactorization (NMF)\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Matrix Factorization (from previous HWs?)\n",
    "# no need to use it in the assignment anyway. see 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GMF_model(num_users, num_items, latent_dim, regs=[[0,0]], activation='sigmoid'):\n",
    "    # Generalized Matrix Factorization\n",
    "    \n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01))  #init = init_normal, \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings \n",
    "    predict_vector = Multiply()([user_latent, item_latent]) #merge([user_latent, item_latent], mode = 'mul')\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation=activation, kernel_initializer='lecun_uniform', name = 'prediction')(predict_vector)\n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_MLP_model(num_users, num_items, latent_dim, regs=[[0,0],0,0], layers = [20,10], activation='sigmoid'):\n",
    "    # Multi-Layer Perceptron\n",
    "    \n",
    "    assert len(layers) + 1 == len(regs), 'the number of regs is equal to number of layers + the embedding layer'\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    \n",
    "    # Concatenation of embedding layers\n",
    "    vector = Concatenate(axis=-1)([user_latent, item_latent])#merge([user_latent, item_latent], mode = 'concat')\n",
    "    \n",
    "    # MLP layers\n",
    "    for idx in range(num_layer):\n",
    "        layer = Dense(layers[idx], kernel_regularizer = l2(regs[idx+1]), activation='relu', name = 'layer%d' %idx)\n",
    "        vector = layer(vector)\n",
    "        \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation=activation, kernel_initializer='lecun_uniform', name = 'prediction')(vector)\n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_NMF_model(num_users, num_items, latent_dim_GMF, latent_dim_MLP, reg_GMF=[[0,0]], regs_MLP=[[0,0],0,0], layers=[20,10], activation='sigmoid'):\n",
    "    # Neural matrix factorization\n",
    "    assert len(layers) + 1 == len(regs_MLP), 'the number of regs is equal to number of layers + the embedding layer'\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    # Embedding layer\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim_GMF, name = 'MF_user_embedding',\n",
    "                                   embeddings_regularizer = l2(reg_GMF[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim_GMF, name = 'MF_item_embedding',\n",
    "                                   embeddings_regularizer = l2(reg_GMF[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01))  #init = init_normal, \n",
    "    \n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim_MLP, name = 'MLP_user_embedding',\n",
    "                                   embeddings_regularizer = l2(regs_MLP[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim_MLP, name = 'MLP_item_embedding',\n",
    "                                   embeddings_regularizer = l2(regs_MLP[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    \n",
    "    # MF part\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    mf_vector = Multiply()([mf_user_latent, mf_item_latent]) #merge([mf_user_latent, mf_item_latent], mode = 'mul') # element-wise multiply\n",
    "\n",
    "    # MLP part\n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    mlp_vector = Concatenate(axis=-1)([mlp_user_latent, mlp_item_latent])#merge([mlp_user_latent, mlp_item_latent], mode = 'concat')\n",
    "    \n",
    "    for idx in range(num_layer):\n",
    "        layer =  Dense(layers[idx], kernel_regularizer = l2(regs_MLP[idx+1]), activation='tanh', name = 'layer%d' %idx)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    # Concatenate MF and MLP parts\n",
    "    predict_vector = Concatenate(axis=-1)([mf_vector, mlp_vector])\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation=activation, kernel_initializer='lecun_uniform', name = \"prediction\")(predict_vector)    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factors = 8 # size of embedding size. Can be split to 4 different params potentially.\n",
    "num_negatives = 4 # how many negative samples per positive sample?\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "verbose = 1\n",
    "write_model=False\n",
    "topK = 10 # used to evaluate the model. Top K recommendations are used.\n",
    "evaluation_threads = 1 \n",
    "model_out_file = 'Pretrain/%s_GMF_%d_%d.h5' %(dataset, num_factors, time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models\n",
    "mlp_model = get_MLP_model(num_users, num_items, num_factors, regs = [[0,0],0,0,0], layers = [32,16,8])\n",
    "gmf_model = get_GMF_model(num_users, num_items, num_factors, regs = [[0,0]])\n",
    "nmf_model = get_NMF_model(num_users, num_items, latent_dim_GMF=num_factors, latent_dim_MLP=num_factors, reg_GMF=[[0,0]], regs_MLP=[[0,0],0,0,0], layers=[32,16,8])\n",
    "\n",
    "mlp_model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
    "gmf_model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
    "nmf_model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Train and evaluate the recommendations accuracy of three models: \n",
    "- MF or GMF\n",
    "- MLP\n",
    "- NMF\n",
    "\n",
    "Compare the learning curve and recommendations accuracy using NDCG and MRR metrics with cutoff values of 5 and 10.   \n",
    "Discuss the comparison.\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phL8meRGnql2"
   },
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq # for retrieval topK\n",
    "import multiprocessing\n",
    "from time import time\n",
    "#from numba import jit, autojit\n",
    "\n",
    "\n",
    "def evaluate_model(model, testRatings, testNegatives, K, num_thread):\n",
    "    \"\"\"\n",
    "    Evaluate the performance (MRR, NDCG) of top-K recommendation\n",
    "    Return: score of each test rating.\n",
    "    \"\"\"\n",
    "    mrrs = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    for idx in range(len(_testRatings)):\n",
    "        (mrr, ndcg) = eval_one_rating(idx)\n",
    "        mrrs.append(mrr)\n",
    "        ndcgs.append(ndcg)\n",
    "    \n",
    "    return (mrrs, ndcgs)\n",
    "\n",
    "\n",
    "########## Maybe need to change like the \"maybe\" file\n",
    "def eval_one_rating(idx):\n",
    "    rating = _testRatings[idx]\n",
    "    items = _testNegatives[idx]\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    items.append(gtItem)\n",
    "    # Get prediction scores\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype = 'int32')\n",
    "    predictions = _model.predict([users, np.array(items)], \n",
    "                                 batch_size=100, verbose=0)\n",
    "    \n",
    "    for i in range(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = predictions[i]\n",
    "    \n",
    "    items.pop()\n",
    "    \n",
    "    # Evaluate top rank list\n",
    "    ranklist = heapq.nlargest(_K, map_item_score, key=map_item_score.get)\n",
    "    mrr = getMRR(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    \n",
    "    return (mrr, ndcg)\n",
    "\n",
    "\n",
    "def getMRR(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        \n",
    "        if item == gtItem:\n",
    "            return 1/(i+1)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        \n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JfUQa6wDf_21",
    "outputId": "f470834c-b8c3-47b2-d2bc-f4ea34f56d0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: HR = 0.0892, NDCG = 0.0402\t [238.8 s]\n"
     ]
    }
   ],
   "source": [
    "models = [['MLP', mlp_model], ['GMF', gmf_model], ['NMF', nmf_model]]\n",
    "\n",
    "for model in models:\n",
    "    # Init performance\r",
    "    \n",
    "t1 = time()    \r",
    "mrrsts, ndcgs) = evaluate_model(mod[1]el, testRatings, testNegatives, topK, evaluation_threads    mrr\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mea\n",
    "    n    ()\r\n",
    "prfi{model[0]} nt('InMRR: H{mrr}%.4f, NDC{ndcg}]Time = {(time()-t1)/60}')1))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. How the values of MRR and NDCG are differ from the results you got in the previous exercises which implemented the explicit recommendation approach. \n",
    "What are the difference in preparing the dataset for evaluation.\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. How will you measure item similarity using the NeuMF model?\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 3: Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. One of the enhancements presented in the Neural Collaborative Filtering paper is the usage of probabilistic activation function (the sigmoid) and binary cross entropy loss function.   \n",
    "\n",
    "Select one of the models you implemented in question 2 and change the loss function to a Mean Squared Error and the activation function of the last layer to RELU.   \n",
    "\n",
    "Train the model and evaluate it in a similar way to what you did in question 2. \n",
    "Compare the results and discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-1ff5a7b22abc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model_3a = get_MLP_model(num_users, num_items, latent_dim_GMF=num_factors, latent_dim_MLP=num_factors, \n\u001b[0m\u001b[0;32m      2\u001b[0m                          \u001b[0mreg_GMF\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregs_MLP\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                          activation='relu')\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_3a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_users' is not defined"
     ]
    }
   ],
   "source": [
    "model_3a = get_MLP_model(num_users, num_items, num_factors, regs = [[0,0],0,0,0], layers = [32,16,8], activation='relu')\n",
    "model_3a.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "print(model_3a.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
