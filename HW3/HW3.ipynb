{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations Systems\n",
    "## Homework 3: Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit your solution in the form of an Jupyter notebook file (with extension ipynb).   \n",
    "Images of graphs or tables should be submitted as PNG or JPG files.   \n",
    "The code used to answer the questions should be included, runnable and documented in the notebook.   \n",
    "Python 3.6 should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this homework is to let you understand the concept of  recommendations based on implicit data which is very common in real life, and learn how ‘Deep neural networks’ components can be used to implement a collaborative filtering and hybrid approach recommenders.  \n",
    "Implementation example is presented in the <a href='https://colab.research.google.com/drive/1v72_zpCObTFMbNnQXUknoQVXR1vBRX6_?usp=sharing'>NeuralCollaborativeFiltering_Implicit</a> notebook in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset based on the <a href='https://grouplens.org/datasets/movielens/1m/'>MovieLens 1M rating dataset</a> after some pre-processing to adapt it to an implicit feedback use case scenario.  \n",
    "You can download the dataset used by <a href='https://github.com/hexiangnan/neural_collaborative_filtering'>this implementation</a> of the paper Neural Collaborative Filtering or from the NeuralCollaborativeFiltering_implicit notebook in Moodle.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from numpy.linalg import solve\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. This implementation contains one file for training and two files for testing:\n",
    "- ml-1m.train.rating\n",
    "- ml-1m.test.rating\n",
    "- ml-1m.test.negative\n",
    "\n",
    "**Explain** the role and structure of each file and how it was created from the original MovieLens 1M rating dataset.\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ml-1m.train.rating:\n",
    "- Training file\n",
    "- Each line is a training instance: userID\\t itemID\\t rating\\t timestamp (if exists)\n",
    "\n",
    "ml-1m.test.rating:\n",
    "- Test file (positive instances)\n",
    "- Each line is a testing instance: userID\\t itemID\\t rating\\t timestamp (if exists)\n",
    "\n",
    "ml-1m.test.negative:\n",
    "- Test file (negative instances)\n",
    "- Each line corresponds to the line of test.rating, containing 99 negative samples\n",
    "- Each line is in the format: (userID,itemID)\\t negativeItemID1\\t negativeItemID2..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. **Explain** how the training dataset is created.\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. **Explain** how the test dataset is created.\n",
    "\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 2: Neural Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Build the following four models using the neural collaborative filtering approach: \n",
    "- Matrix Factorization (MF)\n",
    "- Multi layer perceptron (MLP)\n",
    "- Generalized Matrix Factorization (GMF) \n",
    "- NeuroMatrixFactorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Train and evaluate the recommendations accuracy of three models: \n",
    "- MF or GMF\n",
    "- MLP\n",
    "- NMF\n",
    "\n",
    "Compare the learning curve and recommendations accuracy using NDCG and MRR metrics with cutoff values of 5 and 10.   \n",
    "Discuss the comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. How the values of MRR and NDCG are differ from the results you got in the previous exercises which implemented the explicit recommendation approach. \n",
    "What are the difference in preparing the dataset for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. How will you measure item similarity using the NeuMF model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 3: Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. One of the enhancements presented in the Neural Collaborative Filtering paper is the usage of probabilistic activation function (the sigmoid) and binary cross entropy loss function.   \n",
    "\n",
    "Select one of the models you implemented in question 2 and change the loss function to a Mean Squared Error and the activation function of the last layer to RELU.   \n",
    "\n",
    "Train the model and evaluate it in a similar way to what you did in question 2. \n",
    "Compare the results and discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from HW1\n",
    "def get_mse(pred, actual):\n",
    "    I = actual != 0  # Indicator function which is zero for missing data\n",
    "    ME = I * (actual - pred)  # Errors between real and predicted ratings\n",
    "    MSE = ME**2\n",
    "    return np.sum(MSE)/np.sum(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
