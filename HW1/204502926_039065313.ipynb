{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDs:\n",
    "ID 1: 204502926 <br>\n",
    "ID 2: 039065313"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from numpy.linalg import solve\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Data exploration (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_histogram(dataset, attribute, bins=25, bar_color='#3498db', edge_color='#2980b9', title='Title', xlab='X', ylab='Y', sort_index=False): \n",
    "    fig, ax = plt.subplots(figsize=(9, 7))\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_title(title, fontsize=24, pad=5, bbox={'facecolor':'k', 'pad':5}, color='w')\n",
    "    ax.set_xlabel(xlab, fontsize=16, labelpad=10)\n",
    "    ax.set_ylabel(ylab, fontsize=16, labelpad=20)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.hist(dataset[attribute], bins=50, color=bar_color, ec=edge_color, linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  item_id  rating  timestamp\n",
      "0      196      242       3  881250949\n",
      "1      186      302       3  891717742\n",
      "2       22      377       1  878887116\n",
      "3      244       51       2  880606923\n",
      "4      166      346       1  886397596\n",
      "Sparsity: 0.9369533063577546\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the dataset sparsity, distribution of number of ratings as well as the average rating value per user\\item. \n",
    "# Include additional exploration you find relevant to questions 2 and 3.\n",
    "# Discuss your insights and possible challenges related to the prediction task described in question 2.\n",
    "\n",
    "\n",
    "# Remember that sparsity is calculated by the number of cells in a matrix that contain a rating divided by the total number of values that matrix could hold given the number of users and items (movies). \n",
    "# In other words, dividing the number of ratings present in the matrix by the product of users and movies in the matrix and subtracting that from 1 will give us the sparsity or the percentage of the ratings matrix that is empty.\n",
    "\n",
    "file_path = 'ml-100k/u.data' # grouplens.org/datasets/movielens/100k\n",
    "\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "data = pd.read_csv(file_path, sep='\\t', names=names)\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "all_users_ids = data.user_id.unique()\n",
    "all_items_ids = data.item_id.unique()\n",
    "ratings_count = data.shape[0] # 100,000\n",
    "users_count = len(all_users_ids)\n",
    "items_count = len(all_items_ids)\n",
    "sparsity = 1 - (ratings_count / (users_count * items_count))\n",
    "print(f'Sparsity: {sparsity}')\n",
    "print()\n",
    "\n",
    "\n",
    "make_histogram(data, 'rating', title='Distribution of number of ratings', xlab='Rating', ylab='# of items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that **'4'** is the dominant rating (34,174 ratings)<br>\n",
    "The average rating is: $\\frac{\\sum_{i=1}^n(i * y(i))}{|S|}$, where i is a rating value (1-5), y(i) is the amount of ratings per user/item, and |S| is the size of the dataset (100K).<br>\n",
    "We can also see that the users are more prone to give a perfect rating ('5') rather than < '3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rating is 3.52986\n"
     ]
    }
   ],
   "source": [
    "average = sum(data['rating']) / len(data['rating'])\n",
    "print(f'The average rating is {average}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 2: Matrix factorization model implementation and evaluation (60 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rating_matrix_from_raw_data(df):\n",
    "    ratings = np.zeros((all_users_ids.shape[0], all_items_ids.shape[0]))\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        ratings[row[1]-1][row[2]-1] = row[3]  \n",
    "        \n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = create_rating_matrix_from_raw_data(data)\n",
    "# ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ml-100k/u1.base\n",
      "./ml-100k/u2.base\n",
      "(943, 1682) (943, 1682)\n",
      "(943, 1682) (943, 1682)\n"
     ]
    }
   ],
   "source": [
    "def get_5_folds(folds_dir='./ml-100k/'):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     change to 5!!!!!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    number_of_folds = 2\n",
    "    header = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    folds = []\n",
    "    \n",
    "    for i in range(number_of_folds):     \n",
    "        print(f'{folds_dir}u{i+1}.base')\n",
    "        \n",
    "        df_train = pd.read_csv(f'{folds_dir}u{i+1}.base', sep='\\t', names=header)\n",
    "        df_test = pd.read_csv(f'{folds_dir}u{i+1}.test', sep='\\t', names=header)   \n",
    "        \n",
    "        rating_train = create_rating_matrix_from_raw_data(df_train)\n",
    "        rating_test  = create_rating_matrix_from_raw_data(df_test)        \n",
    "           \n",
    "        folds.append((rating_train, rating_test))\n",
    "        \n",
    "    return folds\n",
    "\n",
    "\n",
    "folds = get_5_folds()\n",
    "\n",
    "for fold in folds:\n",
    "    print(fold[0].shape, fold[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_rmse(pred, actual):\n",
    "    # Ignore nonzero terms\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "\n",
    "    result = np.sqrt(mean_squared_error(pred, actual))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_mrr(pred, actual):    \n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.61111111111111105\n",
    "    >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.5\n",
    "    >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.75\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "    matrix = get_relevant_item_matches_matrix_per_user(pred, actual)\n",
    "    matrix = (np.asarray(r).nonzero()[0] for r in matrix)\n",
    "    \n",
    "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in matrix])\n",
    "\n",
    "\n",
    "def sort_user_ratings(row):\n",
    "    '''Find the indices of the maximal values in given array'''\n",
    "    return row.argsort()[-5:][::-1]\n",
    "\n",
    "\n",
    "def get_relevant_item_matches_matrix_per_user(pred_matrix, actual_matrix):\n",
    "    '''\n",
    "    \n",
    "    Params:\n",
    "    ======\n",
    "    RATING_THRESHOLD_VALUE: (int)\n",
    "            A value between 0-5 (inclusive).\n",
    "            The minimal value to indicate that both our prediction and the actual rating recommend an item.\n",
    "            Below that, either the user finds this item relevant and we predicted he wouldn't, or the other way around.\n",
    "    \n",
    "    MAX_COLUMNS_CUTOFF: (int)\n",
    "            A value of 5 or 10 for this exercise.\n",
    "            The top n items to check for relevancy.\n",
    "    '''\n",
    "    RATING_THRESHOLD_VALUE = 3\n",
    "    MAX_COLUMNS_CUTOFF = 5 # or 10\n",
    "\n",
    "    top_values_indices = np.apply_along_axis(sort_user_ratings, 1, actual_matrix)\n",
    "    result = np.zeros_like(top_values_indices)\n",
    "    rows = len(top_values_indices)\n",
    "    columns = np.minimum(len(top_values_indices[0]), MAX_COLUMNS_CUTOFF)\n",
    "    \n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            match = actual_matrix[i][top_values_indices[i][j]] >= RATING_THRESHOLD_VALUE and pred_matrix[i][top_values_indices[i][j]] >= RATING_THRESHOLD_VALUE\n",
    "\n",
    "            if match:\n",
    "                result[i][j] = 1\n",
    "                break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# actual = np.array([\n",
    "#     [0, 4, 5, 1, 3, 5], \n",
    "#     [0, 5, 4, 1, 4, 4], \n",
    "#     [4, 4, 0, 3, 3, 3]])\n",
    "\n",
    "# pred = np.array([\n",
    "#     [0, 2, 2, 1, 3, 2], \n",
    "#     [0, 3, 5, 1, 5, 5], \n",
    "#     [5, 4, 0, 3, 3, 3]])\n",
    "\n",
    "# print(get_mrr(pred, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 5., 0., 3.],\n",
       "       [3., 3., 1., 3.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_RATING_VALUE = 5\n",
    "np.random.randint(MAX_RATING_VALUE + 1, size=(2, 4)).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitMFSGD():\n",
    "    def __init__(self, \n",
    "                 ratings,\n",
    "                 k_factors = 40,\n",
    "                 item_fact_reg = 0.0, \n",
    "                 user_fact_reg = 0.0,\n",
    "                 item_bias_reg = 0.0,\n",
    "                 user_bias_reg = 0.0,\n",
    "                 verbose = False,\n",
    "                 is_bias_only=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty entries in a matrix. \n",
    "        The terminology assumes a ratings matrix which is ~ user x item.\n",
    "        \n",
    "        (To avoid overfitting we use regularization)\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        ratings: (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        k_factors: (int)\n",
    "            Number of latent factors to use in matrix factorization model\n",
    "        \n",
    "        item_fact_reg: (float)\n",
    "            Regularization term for item latent factors\n",
    "            \n",
    "        user_fact_reg: (float)\n",
    "            Regularization term for user latent factors\n",
    "            \n",
    "        item_bias_reg: (float)\n",
    "            Regularization term for item biases\n",
    "        \n",
    "        user_bias_reg: (float)\n",
    "            Regularization term for user biases\n",
    "        \n",
    "        verbose: (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.users_count = users_count\n",
    "        self.items_count = items_count\n",
    "        self.k_factors = k_factors\n",
    "        self.item_fact_reg = item_fact_reg\n",
    "        self.user_fact_reg = user_fact_reg\n",
    "        self.item_bias_reg = item_bias_reg\n",
    "        self.user_bias_reg = user_bias_reg\n",
    "        \n",
    "        self.sample_row, self.sample_col = self.ratings.nonzero()\n",
    "        self.n_samples = len(self.sample_row)\n",
    "        self._v = verbose\n",
    "        self.is_bias_only = is_bias_only\n",
    "        self.curr_iter = 1\n",
    "        self.user_vecs, self.item_vecs = None, None\n",
    "        self.debug_counter = 0\n",
    "\n",
    "    def calculate_learning_curve(self, iter_array, test, learning_rate):\n",
    "        iter_array.sort()\n",
    "        self.init_for_train(learning_rate)\n",
    "        training_rmse, test_rmse, training_mrr, test_mrr = [], [], [], []\n",
    "        \n",
    "        for n_iter in iter_array:\n",
    "            if self.is_bias_only:\n",
    "                self.print_verbose(f'Bias-only model')\n",
    "            \n",
    "            self.print_verbose(f'k={self.k_factors}, alpha={learning_rate}, '\\\n",
    "                              f'iterations={n_iter}, item_fact_reg={self.item_fact_reg}, '\\\n",
    "                              f'user_fact_reg={self.user_fact_reg}, item_bias_reg={self.item_bias_reg}, '\\\n",
    "                              f'user_bias_reg={self.user_bias_reg}')\n",
    "            self.train(n_iter)\n",
    "            predictions = self.get_prediction_matrix()\n",
    "#             try:\n",
    "            self.evaluate(predictions, test, training_rmse, test_rmse, training_mrr, test_mrr)\n",
    "#             except:\n",
    "#                 print(f'self.global_bias: {self.global_bias}')\n",
    "#                 print(f'self.user_bias[user]: {self.user_bias}')\n",
    "#                 print(f'self.item_bias[item]: {self.item_bias}')\n",
    "#                 print(f'self.debug_counter: {self.debug_counter}')\n",
    "#                 print('----')\n",
    "                \n",
    "        print('============================================')\n",
    "        \n",
    "        return training_rmse, test_rmse, training_mrr, test_mrr\n",
    "    \n",
    "    \n",
    "    def init_for_train(self, learning_rate=0.1):        \n",
    "        # initialize latent vectors\n",
    "        # Approximate rating matrix by product of lower rank matrix\n",
    "        self.user_vecs = np.random.randint(MAX_RATING_VALUE + 1, size=(self.users_count, self.k_factors)).astype(float)\n",
    "        self.item_vecs = np.random.randint(MAX_RATING_VALUE + 1, size=(self.items_count, self.k_factors)).astype(float)\n",
    "#         self.item_vecs = np.random.random((self.items_count, self.k_factors))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.user_bias = np.zeros(self.users_count)\n",
    "        self.item_bias = np.zeros(self.items_count)\n",
    "        self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "\n",
    "    def train(self, n_iter):\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. \n",
    "        Can be called multiple times for further training.\n",
    "        \"\"\"\n",
    "        while self.curr_iter <= n_iter:\n",
    "            if self.curr_iter % 10 == 0 and n_iter > 10 and self._v:\n",
    "                print(f'\\tStill running...')\n",
    "                print(f'\\tCurrent iteration: {self.curr_iter}')\n",
    "                \n",
    "            self.training_indices = np.arange(self.n_samples)\n",
    "            np.random.shuffle(self.training_indices)\n",
    "            self.perform_sgd()\n",
    "            self.curr_iter += 1\n",
    "            \n",
    "    def predict(self, user, item):\n",
    "        \"\"\"\n",
    "        Single user and item prediction\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.debug_counter += 1\n",
    "            biases = self.global_bias + self.user_bias[user] + self.item_bias[item]\n",
    "            prediction_value = biases\n",
    "\n",
    "            if not self.is_bias_only: # bias-only model\n",
    "                prediction_value += self.user_vecs[user, :].dot(self.item_vecs[item, :].T)\n",
    "        except:\n",
    "            print(f'self.global_bias: {self.global_bias}')\n",
    "            print(f'self.user_bias[user]: {self.user_bias[user]}')\n",
    "            print(f'self.item_bias[item]: {self.item_bias[item]}')\n",
    "            print(f'prediction_value: {prediction_value}')\n",
    "            print(f'self.debug_counter: {self.debug_counter}')\n",
    "            print('----')\n",
    "        \n",
    "        return prediction_value\n",
    "    \n",
    "    def perform_sgd(self):\n",
    "        for idx in self.training_indices:\n",
    "            user = self.sample_row[idx]\n",
    "            item = self.sample_col[idx]\n",
    "            prediction = self.predict(user, item)\n",
    "            actual_rating = self.ratings[user, item] # get actual rating from the dataset's ratings array \n",
    "            error = actual_rating - prediction\n",
    "            \n",
    "            # Update biases\n",
    "            self.user_bias[user] += (self.learning_rate * (error - self.user_bias_reg * self.user_bias[user]))\n",
    "            self.item_bias[item] += (self.learning_rate * (error - self.item_bias_reg * self.item_bias[item]))\n",
    "            \n",
    "            # Update latent factors\n",
    "            self.user_vecs[user, :] += self.learning_rate * (error * self.item_vecs[item, :] - self.user_fact_reg * self.user_vecs[user,:])\n",
    "            self.item_vecs[item, :] += self.learning_rate * (error * self.user_vecs[user, :] - self.item_fact_reg * self.item_vecs[item,:])\n",
    "    \n",
    "    def get_prediction_matrix(self):\n",
    "        \"\"\"\n",
    "        Predict ratings for every user and item\n",
    "        \"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        \n",
    "        for user in range(self.user_vecs.shape[0]):\n",
    "            for item in range(self.item_vecs.shape[0]):\n",
    "                predictions[user, item] = self.predict(user, item)\n",
    "                \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, predictions, test, training_rmse, test_rmse, training_mrr, test_mrr):\n",
    "        original = copy.deepcopy(predictions)\n",
    "        training_rmse.append(get_rmse(predictions, self.ratings))\n",
    "        test_rmse.append(get_rmse(predictions, test))\n",
    "\n",
    "        training_mrr.append(get_mrr(predictions, self.ratings))\n",
    "        test_mrr.append(get_mrr(predictions, test))\n",
    "        \n",
    "        self.print_verbose(f'Training RMSE = {training_rmse[-1]}')\n",
    "        self.print_verbose(f'Test RMSE = {test_rmse[-1]}')\n",
    "        \n",
    "        self.print_verbose(f'Training MRR = {training_mrr[-1]}')\n",
    "        self.print_verbose(f'Test MRR = {test_mrr[-1]}')\n",
    "\n",
    "    def print_verbose(self, msg):\n",
    "        if self._v:\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitMFALS:\n",
    "    def __init__(self,\n",
    "                 ratings,\n",
    "                 k_factors=40,\n",
    "                 item_reg=0.0,\n",
    "                 user_reg=0.0,\n",
    "                 verbose=False):\n",
    "        self.ratings = ratings\n",
    "        self.users_count = users_count\n",
    "        self.items_count = items_count        \n",
    "        self.k_factors = k_factors\n",
    "        self.item_reg = item_reg\n",
    "        self.user_reg = user_reg\n",
    "        self._v = verbose\n",
    "        self.curr_iter = 1\n",
    "\n",
    "    def calculate_learning_curve(self, iter_array, test):\n",
    "        iter_array.sort()\n",
    "        self.init_for_train()\n",
    "        training_rmse, test_rmse, training_mrr, test_mrr = [], [], [], []\n",
    "        \n",
    "        for n_iter in iter_array:            \n",
    "            self.print_verbose(f'k={self.k_factors}, '\\\n",
    "                               f'iterations={n_iter}, '\\\n",
    "                               f'item_reg={self.item_reg}, '\\\n",
    "                               f'user_reg={self.user_reg}')\n",
    "            self.train(n_iter)\n",
    "            predictions = self.predict_all()\n",
    "            self.evaluate(predictions, test, training_rmse, test_rmse, training_mrr, test_mrr)  \n",
    "        \n",
    "        print('============================================')\n",
    "        \n",
    "        return training_rmse, test_rmse, training_mrr, test_mrr\n",
    "    \n",
    "    def als_step_user(self):\n",
    "        YTY = self.item_vecs.T.dot(self.item_vecs)\n",
    "        lambdaI = np.eye(YTY.shape[0]) * self.user_reg\n",
    "\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            self.user_vecs[u, :] = solve((YTY + lambdaI), self.ratings[u, :].dot(self.item_vecs))\n",
    "\n",
    "    def als_step_item(self):\n",
    "        XTX = self.user_vecs.T.dot(self.user_vecs)\n",
    "        lambdaI = np.eye(XTX.shape[0]) * self.item_reg\n",
    "\n",
    "        for i in range(self.item_vecs.shape[0]):\n",
    "            self.item_vecs[i, :] = solve((XTX + lambdaI), self.ratings[:, i].T.dot(self.user_vecs))\n",
    "\n",
    "    def init_for_train(self):\n",
    "        # initialize latent vectors\n",
    "        self.user_vecs = np.random.normal(scale=1. / self.k_factors,\n",
    "                                          size=(self.users_count, self.k_factors))\n",
    "        self.item_vecs = np.random.normal(scale=1. / self.k_factors,\n",
    "                                          size=(self.items_count, self.k_factors))\n",
    "        print(f'user_vecs: {self.user_vecs.shape}, item_vecs: {self.item_vecs.shape}')\n",
    "\n",
    "    def train(self, n_iter):\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. \n",
    "        Can be called multiple times for further training.\n",
    "        \"\"\"\n",
    "        while self.curr_iter <= n_iter:\n",
    "            if self.curr_iter % 10 == 0 and n_iter > 10 and self._v:\n",
    "                print(f'\\tStill running...')\n",
    "                print(f'\\tCurrent iteration: {self.curr_iter}')\n",
    "                \n",
    "            self.als_step_user()\n",
    "            self.als_step_item()\n",
    "            self.curr_iter += 1\n",
    "            \n",
    "    def predict(self, u, i):\n",
    "        return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "\n",
    "    def predict_all(self):\n",
    "        predictions = np.zeros((self.user_vecs.shape[0],\n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "        return predictions\n",
    "                       \n",
    "    def evaluate(self, predictions, test, training_rmse, test_rmse, training_mrr, test_mrr):\n",
    "        training_rmse.append(get_rmse(predictions, self.ratings))\n",
    "        test_rmse.append(get_rmse(predictions, test))\n",
    "\n",
    "        training_mrr.append(get_mrr(predictions, self.ratings))\n",
    "        test_mrr.append(get_mrr(predictions, test))\n",
    "        \n",
    "        self.print_verbose(f'Training RMSE = {training_rmse[-1]}')\n",
    "        self.print_verbose(f'Test RMSE = {test_rmse[-1]}')\n",
    "        \n",
    "        self.print_verbose(f'Training MRR = {training_mrr[-1]}')\n",
    "        self.print_verbose(f'Test MRR = {test_mrr[-1]}')\n",
    "        \n",
    "    def print_verbose(self, msg):\n",
    "        if self._v:\n",
    "            print(msg)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from itertools import combinations, combinations_with_replacement, permutations\n",
    "\n",
    "def permutation_regularization_generator(base_dict):\n",
    "#     regularization_parameters_values = [0.01, 0.1, 1.0]\n",
    "    regularization_parameters_values = [0.01]\n",
    "    regularization_permutations = set()\n",
    "    number_of_regularization_params = len(base_dict)\n",
    "    \n",
    "    for seq in list(combinations_with_replacement(regularization_parameters_values, number_of_regularization_params)):\n",
    "        for perm_seq in permutations(seq, number_of_regularization_params):\n",
    "            regularization_permutations.add(perm_seq)\n",
    "\n",
    "    regularization_parameters_dict_list = []      \n",
    "    keys = base_dict.keys()\n",
    "    \n",
    "    for perm in regularization_permutations:\n",
    "        curr_values_dict = copy.deepcopy(base_dict)\n",
    "        \n",
    "        for idx, key in enumerate(keys):\n",
    "            curr_values_dict[key] = perm[idx]\n",
    "        \n",
    "        regularization_parameters_dict_list.append(curr_values_dict)\n",
    "        \n",
    "    return regularization_parameters_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list_of_regularization_parameters_values(model_type):\n",
    "    if model_type.lower() == \"als\":\n",
    "        base_dict = {'item_reg': 0.0, 'user_reg':0.0}\n",
    "    else: # model type == \"sgd\"\n",
    "        base_dict = {'item_fact_reg': 0.0, 'user_fact_reg': 0.0, 'item_bias_reg': 0.0, 'user_bias_reg':0.0}\n",
    "        \n",
    "    return permutation_regularization_generator(base_dict)        \n",
    "\n",
    "    \n",
    "def als_model_provider(**params):\n",
    "    return ExplicitMFALS(**params)\n",
    "\n",
    "\n",
    "def sgd_model_provider(**params):\n",
    "    return ExplicitMFSGD(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:10, b:20, c:35, d:45\n"
     ]
    }
   ],
   "source": [
    "# CODE EXAMPLE ON HOW I PASSED PARAMS:\n",
    "\n",
    "def y(a=0, b=1, c=2, d=3):\n",
    "    print(f'a:{a}, b:{b}, c:{c}, d:{d}')\n",
    "    \n",
    "def x(**params):\n",
    "    y(**params)\n",
    "    \n",
    "last_params = {'c': 35, 'd': 45}\n",
    "x(a=10,b=20,**last_params)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_learning_curve(training_data, test_data, model_provider, model_type=\"sgd\"):\n",
    "    regularization_parameters_values = generate_list_of_regularization_parameters_values(model_type)    \n",
    "    perm_count = 1\n",
    "    params_and_results = []\n",
    "    \n",
    "    if model_type.lower() == \"als\":\n",
    "        hyper_param_learning_iter_num = len(k_options)*len(regularization_parameters_values)\n",
    "    else: # sgd\n",
    "        hyper_param_learning_iter_num = len(k_options)*len(learning_rate_options)*len(regularization_parameters_values)\n",
    "        \n",
    "    for k in k_options:\n",
    "        if model_type.lower() == \"als\":\n",
    "            for regularization_parameters_value in regularization_parameters_values:    \n",
    "                print(f'iteration {perm_count}/{hyper_param_learning_iter_num}')\n",
    "                model = model_provider(ratings=training_data, k_factors=k, verbose=True, **regularization_parameters_value)\n",
    "                training_rmse, test_rmse, training_mrr, test_mrr = model.calculate_learning_curve(iter_array, test_data)\n",
    "                curr_run_params_and_results = {\"k\": k, \"training_rmse\": training_rmse, \"test_rmse\": test_rmse, \"training_mrr\": training_mrr, \"test_mrr\": test_mrr}\n",
    "                params_and_results.append({**curr_run_params_and_results, **regularization_parameters_value})    \n",
    "                perm_count += 1\n",
    "        else: # sgd\n",
    "            for learning_rate in learning_rate_options:\n",
    "                for regularization_parameters_value in regularization_parameters_values:    \n",
    "                    print(f'iteration {perm_count}/{hyper_param_learning_iter_num} ')\n",
    "                    model = model_provider(ratings=training_data, k_factors=k, verbose=True, is_bias_only=False, **regularization_parameters_value)\n",
    "                    training_rmse, test_rmse, training_mrr, test_mrr = model.calculate_learning_curve(iter_array, test_data, learning_rate)\n",
    "                    curr_run_params_and_results = {\"k\": k, \"learning_rate\": learning_rate, \"training_rmse\": training_rmse, \"test_rmse\": test_rmse, \"training_mrr\": training_mrr, \"test_mrr\": test_mrr}\n",
    "                    params_and_results.append({**curr_run_params_and_results, **regularization_parameters_value})    \n",
    "                    perm_count += 1\n",
    "                    \n",
    "    return params_and_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OFFICIAL AND FINAL PARAMETERS FOR SUBMISSION\n",
    "k_options = [5, 10, 20]\n",
    "learning_rate_options = [0.001, 0.01, 0.1]\n",
    "iter_array = [1, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# DEMO AND SMALLER PARAMETERS - NOT FOR SUBMISSION\n",
    "k_options = [5, 10]\n",
    "learning_rate_options = [0.1]\n",
    "iter_array = [1, 10, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_permutations_string(model_type=\"sgd\"):\n",
    "    regularization_parameters_values = generate_list_of_regularization_parameters_values(model_type)\n",
    "    print(f'Total of {len(regularization_parameters_values)} regularization parameters permutations')\n",
    "\n",
    "    if model_type.lower() == \"als\":\n",
    "        permutations_string = \"(|k's| * |reg's|^2)\"\n",
    "        hyper_param_learning_iter_num = len(k_options)*len(regularization_parameters_values)\n",
    "    else: # sgd\n",
    "        permutations_string = \"(|k's| * |alphas| * |reg's|^4)\"\n",
    "        hyper_param_learning_iter_num = len(k_options)*len(learning_rate_options)*len(regularization_parameters_values)\n",
    "\n",
    "    print(f'For each fold - {hyper_param_learning_iter_num} permutations (runs) on the model parameters {permutations_string}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test/Collect ALS 5-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_folds_results = []\n",
    "\n",
    "get_number_of_permutations_string(model_type=\"als\")\n",
    "    \n",
    "for idx, fold in enumerate(folds, start = 0):\n",
    "    print('============================================')\n",
    "    print(f'Running on fold {idx}/{len(folds) - 1}')\n",
    "    training_data, test_data = fold\n",
    "    params_and_results = calculate_learning_curve(training_data, test_data, model_provider=als_model_provider, model_type=\"als\")\n",
    "    als_folds_results.append(params_and_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test/Collect SGD 5-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 1 regularization parameters permutations\n",
      "For each fold - 2 permutations (runs) on the model parameters (|k's| * |alphas| * |reg's|^4)\n",
      "============================================\n",
      "Running on fold 0/1\n",
      "iteration 1/2 \n",
      "k=5, alpha=0.1, iterations=1, item_fact_reg=0.01, user_fact_reg=0.01, item_bias_reg=0.01, user_bias_reg=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\on737092\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:149: RuntimeWarning: overflow encountered in multiply\n",
      "c:\\users\\on737092\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:148: RuntimeWarning: overflow encountered in multiply\n",
      "c:\\users\\on737092\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:149: RuntimeWarning: invalid value encountered in subtract\n",
      "c:\\users\\on737092\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:148: RuntimeWarning: invalid value encountered in add\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-27a54a195189>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Running on fold {idx}/{len(folds) - 1}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mparams_and_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_learning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_provider\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd_model_provider\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sgd\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0msgd_folds_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_and_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-636e6e1c237c>\u001b[0m in \u001b[0;36mcalculate_learning_curve\u001b[1;34m(training_data, test_data, model_provider, model_type)\u001b[0m\n\u001b[0;32m     23\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'iteration {perm_count}/{hyper_param_learning_iter_num} '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_provider\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_factors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_bias_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mregularization_parameters_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                     \u001b[0mtraining_rmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_rmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_mrr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_mrr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_learning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                     \u001b[0mcurr_run_params_and_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"k\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"learning_rate\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"training_rmse\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtraining_rmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"test_rmse\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_rmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"training_mrr\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtraining_mrr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"test_mrr\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_mrr\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                     \u001b[0mparams_and_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcurr_run_params_and_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mregularization_parameters_value\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-d9bd41c24e4c>\u001b[0m in \u001b[0;36mcalculate_learning_curve\u001b[1;34m(self, iter_array, test, learning_rate)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_prediction_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;31m#             try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_rmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_rmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_mrr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_mrr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;31m#             except:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m#                 print(f'self.global_bias: {self.global_bias}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-d9bd41c24e4c>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, predictions, test, training_rmse, test_rmse, training_mrr, test_mrr)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_rmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_rmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_mrr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_mrr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0moriginal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtraining_rmse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_rmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[0mtest_rmse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_rmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-53781264bdc7>\u001b[0m in \u001b[0;36mget_rmse\u001b[1;34m(pred, actual)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mactual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactual\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mactual\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\on737092\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\on737092\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \"\"\"\n\u001b[0;32m    255\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 256\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    257\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[1;32mc:\\users\\on737092\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m     84\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\on737092\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\on737092\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 645\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\on737092\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     97\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                     (type_err,\n\u001b[1;32m---> 99\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m    100\u001b[0m             )\n\u001b[0;32m    101\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "sgd_folds_results = []\n",
    "\n",
    "get_number_of_permutations_string(model_type=\"sgd\")\n",
    "\n",
    "for idx, fold in enumerate(folds, start = 0):\n",
    "    print('============================================')\n",
    "    print(f'Running on fold {idx}/{len(folds) - 1}')\n",
    "    training_data, test_data = fold\n",
    "    params_and_results = calculate_learning_curve(training_data, test_data, model_provider=sgd_model_provider, model_type=\"sgd\")\n",
    "    sgd_folds_results.append(params_and_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def generate_key(dict_item, model_type=\"sgd\"):\n",
    "    if model_type.lower() == \"als\":\n",
    "        key = f\"k_{dict_item['k']}_ir_{dict_item['item_reg']}_ur_{dict_item['user_reg']}\"\n",
    "    else:\n",
    "        key = f\"k_{dict_item['k']}_lr_{dict_item['learning_rate']}_if_{dict_item['item_fact_reg']}_uf_{dict_item['user_fact_reg']}_ib_{dict_item['item_bias_reg']}_ub_{dict_item['user_bias_reg']}\"\n",
    "    \n",
    "    return key\n",
    "\n",
    "\n",
    "def calculate_average(matrix):\n",
    "    '''\n",
    "    Take the mean of each column from a given matrix\n",
    "    '''\n",
    "    return np.mean(matrix, axis=0)\n",
    "\n",
    "\n",
    "def get_average_map(given_map):\n",
    "    for key, value in given_map.items():\n",
    "        matrix = np.array(value)\n",
    "        new_value = calculate_average(matrix)\n",
    "        given_map[key] = new_value\n",
    "        \n",
    "    return given_map\n",
    "\n",
    "\n",
    "def create_error_map(folds_results_list, key_name, model_type=\"sgd\"):\n",
    "    results_map = defaultdict(list)\n",
    "\n",
    "    for fold_result in folds_results_list:\n",
    "        for dict_item in fold_result:\n",
    "            generated_key = generate_key(dict_item, model_type)\n",
    "            current_error_list = dict_item[key_name]\n",
    "            results_map[generated_key].append(current_error_list)\n",
    "            \n",
    "    results_map = get_average_map(results_map)\n",
    "    \n",
    "    return results_map\n",
    "\n",
    "\n",
    "def combine_training_and_test_averages_per_model(folds_results_list, averages_training_map, averages_test_map, key_names, model_type=\"sgd\"):\n",
    "    averaged_folds_results = []\n",
    "    \n",
    "    for fold_result in folds_results_list[0]:\n",
    "        dict_copy = fold_result.copy()\n",
    "        \n",
    "        dict_copy[key_names[0]] = averages_training_map[generate_key(fold_result, model_type)]\n",
    "        dict_copy[key_names[1]] = averages_test_map[generate_key(fold_result, model_type)]\n",
    "        \n",
    "        averaged_folds_results.append(dict_copy)\n",
    "            \n",
    "    return averaged_folds_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo = [\n",
    "#     [\n",
    "#     {'k': 10,\n",
    "#    'learning_rate': 0.001,\n",
    "#    'training_rmse': [0,0,0],\n",
    "#    'test_rmse': [1,1,1],\n",
    "#    'item_fact_reg': 100.0,\n",
    "#    'user_fact_reg': 100.0,\n",
    "#    'item_bias_reg': 100.0,\n",
    "#    'user_bias_reg': 100.0},\n",
    "#        {'k': 20,\n",
    "#    'learning_rate': 0.001,\n",
    "#    'training_rmse': [1,2,3],\n",
    "#    'test_rmse': [0,0,0],\n",
    "#    'item_fact_reg': 100.0,\n",
    "#    'user_fact_reg': 100.0,\n",
    "#    'item_bias_reg': 100.0,\n",
    "#    'user_bias_reg': 100.0}\n",
    "#        ],\n",
    "#     [\n",
    "#     {'k': 10,\n",
    "#    'learning_rate': 0.001,\n",
    "#    'training_rmse': [1,1,1],\n",
    "#    'test_rmse': [2,2,2],\n",
    "#    'item_fact_reg': 100.0,\n",
    "#    'user_fact_reg': 100.0,\n",
    "#    'item_bias_reg': 100.0,\n",
    "#    'user_bias_reg': 100.0},\n",
    "#        {'k': 20,\n",
    "#    'learning_rate': 0.001,\n",
    "#    'training_rmse': [5,5,5],\n",
    "#    'test_rmse': [5,5,5],\n",
    "#    'item_fact_reg': 100.0,\n",
    "#    'user_fact_reg': 100.0,\n",
    "#    'item_bias_reg': 100.0,\n",
    "#    'user_bias_reg': 100.0}\n",
    "#        ],\n",
    "# ]\n",
    "\n",
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============= RMSE\n",
    "training_sgd_averages_map_rmse = create_error_map(sgd_folds_results, 'training_rmse', model_type=\"sgd\")\n",
    "test_sgd_averages_map_rmse = create_error_map(sgd_folds_results, 'test_rmse', model_type=\"sgd\")\n",
    "\n",
    "training_als_averages_map_rmse = create_error_map(als_folds_results, 'training_rmse', model_type=\"als\")\n",
    "test_als_averages_map_rmse = create_error_map(als_folds_results, 'test_rmse', model_type=\"als\")\n",
    "\n",
    "\n",
    "# ============= MRR\n",
    "\n",
    "# training_sgd_mrr\n",
    "# test_sgd_mrr\n",
    "training_als_averages_map_mrr = create_error_map(als_folds_results, 'training_mrr', model_type=\"als\")\n",
    "test_als_averages_map_mrr = create_error_map(als_folds_results, 'test_mrr', model_type=\"als\")\n",
    "\n",
    "# ============= nDCG\n",
    "\n",
    "\n",
    "print(training_sgd_averages_map_rmse)\n",
    "print(test_sgd_averages_map_rmse)\n",
    "print()\n",
    "print(training_als_averages_map_rmse)\n",
    "print(test_als_averages_map_rmse)\n",
    "print()\n",
    "print(training_als_averages_map_mrr)\n",
    "print(test_als_averages_map_mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_names_array=['training_rmse', 'test_rmse', 'training_mrr', 'test_mrr']\n",
    "sgd_averages_result_rmse = combine_training_and_test_averages_per_model(sgd_folds_results, training_sgd_averages_map_rmse, test_sgd_averages_map_rmse, key_names=['training_rmse', 'test_rmse'], model_type=\"sgd\")\n",
    "print(sgd_averages_result_rmse)\n",
    "print()\n",
    "\n",
    "als_averages_result = combine_training_and_test_averages_per_model(als_folds_results, training_als_averages_map_rmse, test_als_averages_map_rmse, key_names=key_names_array, model_type=\"als\")\n",
    "print(als_averages_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "def plot_learning_curve_list(iter_array, predictions, title, error_method, mse_extractor, model_type=\"als\"):\n",
    "    plt.figure(num=None, figsize=(20, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for prediction in predictions:\n",
    "        k = prediction[\"k\"]\n",
    "        \n",
    "        if model_type.lower() == \"als\":\n",
    "            ir, ur = prediction['item_reg'], prediction['user_reg']\n",
    "            plt.plot(iter_array, mse_extractor(prediction), label=f\"{title} k={k}, item_reg={ir}, user_reg={ur}\", linewidth=5)\n",
    "        else:\n",
    "            learning_rate = prediction[\"learning_rate\"]\n",
    "            if_reg, uf_reg, ib, ub = prediction['item_fact_reg'], prediction['user_fact_reg'], prediction['item_bias_reg'], prediction['user_bias_reg']\n",
    "            plt.plot(iter_array, mse_extractor(prediction), label=f\"{title} k={k}, ={learning_rate}, if={if_reg}, uf={uf_reg}, if={if_reg}, uf={uf_reg}, ib={ib}, ub={ub}\", linewidth=5)\n",
    "    \n",
    "    plt.xticks(fontsize=16);\n",
    "    plt.yticks(fontsize=16);\n",
    "    plt.xlabel('iterations', fontsize=20);\n",
    "    plt.ylabel(f'{error_method} (averaged)', fontsize=20);\n",
    "    plt.title(f'{title} - {model_type.upper()} - ({error_method})', fontsize=30, bbox={'facecolor':'k', 'pad':5}, color='w')\n",
    "    plt.legend(loc='best', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_TRAINING = 'TRAINING'\n",
    "TITLE_TEST = 'TEST'\n",
    "ERROR_METHOD_RMSE = 'RMSE'\n",
    "ERROR_METHOD_MRR = 'MRR'\n",
    "ERROR_METHOD_NDCG = 'NDCG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve_list(iter_array, title=TITLE_TRAINING, error_method = ERROR_METHOD_RMSE, mse_extractor=lambda model: model[\"training_rmse\"], model_type=\"sgd\", predictions=sgd_averages_result_rmse)\n",
    "plot_learning_curve_list(iter_array, title=TITLE_TEST, error_method = ERROR_METHOD_RMSE, mse_extractor=lambda model: model[\"test_rmse\"], model_type=\"sgd\", predictions=sgd_averages_result_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve_list(iter_array, title=TITLE_TRAINING, error_method=ERROR_METHOD_RMSE, mse_extractor=lambda model: model[\"training_rmse\"], model_type=\"als\", predictions=als_averages_result)\n",
    "plot_learning_curve_list(iter_array, title=TITLE_TEST, error_method=ERROR_METHOD_RMSE, mse_extractor=lambda model: model[\"test_rmse\"], model_type=\"als\", predictions=als_averages_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve_list(iter_array, title=TITLE_TRAINING, error_method=ERROR_METHOD_MRR, mse_extractor=lambda model: model[\"training_mrr\"], model_type=\"als\", predictions=als_averages_result)\n",
    "plot_learning_curve_list(iter_array, title=TITLE_TEST, error_method=ERROR_METHOD_MRR, mse_extractor=lambda model: model[\"test_mrr\"], model_type=\"als\", predictions=als_averages_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Matrix factorization  item similarity and model explainability (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
