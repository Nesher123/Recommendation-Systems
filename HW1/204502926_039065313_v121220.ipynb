{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDs:\n",
    "ID 1: 204502926 <br>\n",
    "ID 2: 039065313"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from numpy.linalg import solve\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Data exploration (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_histogram(dataset, attribute, bins=25, bar_color='#3498db', edge_color='#2980b9', title='Title', xlab='X', ylab='Y', sort_index=False): \n",
    "    fig, ax = plt.subplots(figsize=(9, 7))\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_title(title, fontsize=24, pad=5, bbox={'facecolor':'k', 'pad':5}, color='w')\n",
    "    ax.set_xlabel(xlab, fontsize=16, labelpad=10)\n",
    "    ax.set_ylabel(ylab, fontsize=16, labelpad=20)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.hist(dataset[attribute], bins=50, color=bar_color, ec=edge_color, linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  item_id  rating  timestamp\n",
      "0      196      242       3  881250949\n",
      "1      186      302       3  891717742\n",
      "2       22      377       1  878887116\n",
      "3      244       51       2  880606923\n",
      "4      166      346       1  886397596\n",
      "Sparsity: 0.9369533063577546\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the dataset sparsity, distribution of number of ratings as well as the average rating value per user\\item. \n",
    "# Include additional exploration you find relevant to questions 2 and 3.\n",
    "# Discuss your insights and possible challenges related to the prediction task described in question 2.\n",
    "\n",
    "\n",
    "# Remember that sparsity is calculated by the number of cells in a matrix that contain a rating divided by the total number of values that matrix could hold given the number of users and items (movies). \n",
    "# In other words, dividing the number of ratings present in the matrix by the product of users and movies in the matrix and subtracting that from 1 will give us the sparsity or the percentage of the ratings matrix that is empty.\n",
    "\n",
    "file_path = 'ml-100k/u.data' # grouplens.org/datasets/movielens/100k\n",
    "\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "data = pd.read_csv(file_path, sep='\\t', names=names)\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "all_users_ids = data.user_id.unique()\n",
    "all_items_ids = data.item_id.unique()\n",
    "ratings_count = data.shape[0] # 100,000\n",
    "users_count = len(all_users_ids)\n",
    "items_count = len(all_items_ids)\n",
    "sparsity = 1 - (ratings_count / (users_count * items_count))\n",
    "print(f'Sparsity: {sparsity}')\n",
    "print()\n",
    "\n",
    "\n",
    "make_histogram(data, 'rating', title='Distribution of number of ratings', xlab='Rating', ylab='# of items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that **'4'** is the dominant rating (34,174 ratings)<br>\n",
    "The average rating is: $\\frac{\\sum_{i=1}^n(i * y(i))}{|S|}$, where i is a rating value (1-5), y(i) is the amount of ratings per user/item, and |S| is the size of the dataset (100K).<br>\n",
    "We can also see that the users are more prone to give a perfect rating ('5') rather than < '3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rating is 3.52986\n"
     ]
    }
   ],
   "source": [
    "average = sum(data['rating']) / len(data['rating'])\n",
    "print(f'The average rating is {average}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 2: Matrix factorization model implementation and evaluation (60 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rating_matrix_from_raw_data(df):\n",
    "    ratings = np.zeros((all_users_ids.shape[0], all_items_ids.shape[0]))\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        ratings[row[1]-1][row[2]-1] = row[3]  \n",
    "        \n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = create_rating_matrix_from_raw_data(data)\n",
    "# ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ml-100k/u1.base\n",
      "./ml-100k/u2.base\n",
      "(943, 1682) (943, 1682)\n",
      "(943, 1682) (943, 1682)\n"
     ]
    }
   ],
   "source": [
    "def get_5_folds(folds_dir='./ml-100k/'):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     change to 5!!!!!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    number_of_folds = 2\n",
    "    header = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    folds = []\n",
    "    \n",
    "    for i in range(number_of_folds):     \n",
    "        print(f'{folds_dir}u{i+1}.base')\n",
    "        \n",
    "        df_train = pd.read_csv(f'{folds_dir}u{i+1}.base', sep='\\t', names=header)\n",
    "        df_test = pd.read_csv(f'{folds_dir}u{i+1}.test', sep='\\t', names=header)   \n",
    "        \n",
    "        rating_train = create_rating_matrix_from_raw_data(df_train)\n",
    "        rating_test  = create_rating_matrix_from_raw_data(df_test)        \n",
    "           \n",
    "        folds.append((rating_train, rating_test))\n",
    "        \n",
    "    return folds\n",
    "\n",
    "\n",
    "folds = get_5_folds()\n",
    "\n",
    "for fold in folds:\n",
    "    print(fold[0].shape, fold[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_rmse(pred, actual):\n",
    "    # Ignore nonzero terms\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "\n",
    "    result = np.sqrt(mean_squared_error(pred, actual))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_mrr(pred, actual):    \n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.61111111111111105\n",
    "    >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.5\n",
    "    >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.75\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "    matrix = get_relevant_item_matches_matrix_per_user(pred, actual)\n",
    "    matrix = (np.asarray(r).nonzero()[0] for r in matrix)\n",
    "    \n",
    "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in matrix])\n",
    "\n",
    "\n",
    "def sort_user_ratings(row):\n",
    "    '''Find the indices of the maximal values in given array'''\n",
    "    return row.argsort()[-5:][::-1]\n",
    "\n",
    "\n",
    "def get_relevant_item_matches_matrix_per_user(pred_matrix, actual_matrix):\n",
    "    '''\n",
    "    \n",
    "    Params:\n",
    "    ======\n",
    "    RATING_THRESHOLD_VALUE: (int)\n",
    "            A value between 0-5 (inclusive).\n",
    "            The minimal value to indicate that both our prediction and the actual rating recommend an item.\n",
    "            Below that, either the user finds this item relevant and we predicted he wouldn't, or the other way around.\n",
    "    \n",
    "    MAX_COLUMNS_CUTOFF: (int)\n",
    "            A value of 5 or 10 for this exercise.\n",
    "            The top n items to check for relevancy.\n",
    "    '''\n",
    "    RATING_THRESHOLD_VALUE = 3\n",
    "    MAX_COLUMNS_CUTOFF = 5 # or 10\n",
    "\n",
    "    top_values_indices = np.apply_along_axis(sort_user_ratings, 1, actual_matrix)\n",
    "    result = np.zeros_like(top_values_indices)\n",
    "    rows = len(top_values_indices)\n",
    "    columns = np.minimum(len(top_values_indices[0]), MAX_COLUMNS_CUTOFF)\n",
    "    \n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            match = actual_matrix[i][top_values_indices[i][j]] >= RATING_THRESHOLD_VALUE and pred_matrix[i][top_values_indices[i][j]] >= RATING_THRESHOLD_VALUE\n",
    "\n",
    "            if match:\n",
    "                result[i][j] = 1\n",
    "                break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# actual = np.array([\n",
    "#     [0, 4, 5, 1, 3, 5], \n",
    "#     [0, 5, 4, 1, 4, 4], \n",
    "#     [4, 4, 0, 3, 3, 3]])\n",
    "\n",
    "# pred = np.array([\n",
    "#     [0, 2, 2, 1, 3, 2], \n",
    "#     [0, 3, 5, 1, 5, 5], \n",
    "#     [5, 4, 0, 3, 3, 3]])\n",
    "\n",
    "# print(get_mrr(pred, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 5., 0., 3.],\n",
       "       [3., 3., 1., 3.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_RATING_VALUE = 5\n",
    "np.random.randint(MAX_RATING_VALUE + 1, size=(2, 4)).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitMFSGD():\n",
    "    def __init__(self, \n",
    "                 ratings,\n",
    "                 k = 40,\n",
    "                 item_fact_reg = 0.0, \n",
    "                 user_fact_reg = 0.0,\n",
    "                 item_bias_reg = 0.0,\n",
    "                 user_bias_reg = 0.0,\n",
    "                 verbose = False,\n",
    "                 is_bias_only=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty entries in a matrix. \n",
    "        The terminology assumes a ratings matrix which is ~ user x item.\n",
    "        \n",
    "        (To avoid overfitting we use regularization)\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        ratings: (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        k_factors: (int)\n",
    "            Number of latent factors to use in matrix factorization model\n",
    "        \n",
    "        item_fact_reg: (float)\n",
    "            Regularization term for item latent factors\n",
    "            \n",
    "        user_fact_reg: (float)\n",
    "            Regularization term for user latent factors\n",
    "            \n",
    "        item_bias_reg: (float)\n",
    "            Regularization term for item biases\n",
    "        \n",
    "        user_bias_reg: (float)\n",
    "            Regularization term for user biases\n",
    "        \n",
    "        verbose: (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.users_count = users_count\n",
    "        self.items_count = items_count\n",
    "        self.k_factors = k\n",
    "        self.item_fact_reg = item_fact_reg\n",
    "        self.user_fact_reg = user_fact_reg\n",
    "        self.item_bias_reg = item_bias_reg\n",
    "        self.user_bias_reg = user_bias_reg\n",
    "        \n",
    "        self.sample_row, self.sample_col = self.ratings.nonzero()\n",
    "        self.n_samples = len(self.sample_row)\n",
    "        self._v = verbose\n",
    "        self.is_bias_only = is_bias_only\n",
    "        self.curr_iter = 1\n",
    "        self.user_vecs, self.item_vecs = None, None\n",
    "\n",
    "    def calculate_learning_curve(self, iter_array, test, learning_rate, evaluate=True):\n",
    "        self.init_for_train(learning_rate)\n",
    "        training_rmse, test_rmse, training_mrr, test_mrr = [], [], [], []\n",
    "        \n",
    "        if not evaluate:\n",
    "            predictions_list = [None]*len(iter_array)\n",
    "        \n",
    "        for idx, n_iter in enumerate(iter_array):\n",
    "            if self.is_bias_only:\n",
    "                self.print_verbose(f'Bias-only model')\n",
    "            \n",
    "            self.print_verbose(f'k={self.k_factors}, alpha={learning_rate}, '\\\n",
    "                              f'iterations={n_iter}, item_fact_reg={self.item_fact_reg}, '\\\n",
    "                              f'user_fact_reg={self.user_fact_reg}, item_bias_reg={self.item_bias_reg}, '\\\n",
    "                              f'user_bias_reg={self.user_bias_reg}')\n",
    "            self.train(n_iter)\n",
    "            predictions = self.get_prediction_matrix()\n",
    "            \n",
    "            if not evaluate:\n",
    "                predictions_list[idx] = predictions_list                \n",
    "            else:\n",
    "                self.evaluate(predictions, test, training_rmse, test_rmse, training_mrr, test_mrr)\n",
    "                \n",
    "        return predictions_list, training_rmse, test_rmse, training_mrr, test_mrr\n",
    "    \n",
    "    \n",
    "#     def init_for_train(self, learning_rate=0.1):        \n",
    "#         # initialize latent vectors\n",
    "#         # Approximate rating matrix by product of lower rank matrix\n",
    "#         self.user_vecs = np.random.randint(MAX_RATING_VALUE + 1, size=(self.users_count, self.k_factors)).astype(float)\n",
    "#         self.item_vecs = np.random.randint(MAX_RATING_VALUE + 1, size=(self.items_count, self.k_factors)).astype(float)\n",
    "# #         self.item_vecs = np.random.random((self.items_count, self.k_factors))\n",
    "        \n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.user_bias = np.zeros(self.users_count)\n",
    "#         self.item_bias = np.zeros(self.items_count)\n",
    "#         self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "    def init_for_train(self, learning_rate=0.1):        \n",
    "        # initialize latent vectors\n",
    "        # Approximate rating matrix by product of lower rank matrix\n",
    "        self.user_vecs = np.random.normal(scale=1./self.k_factors, size=(self.users_count, self.k_factors))\n",
    "        self.item_vecs = np.random.normal(scale=1./self.k_factors, size=(self.items_count, self.k_factors))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.user_bias = np.zeros(self.users_count)\n",
    "        self.item_bias = np.zeros(self.items_count)\n",
    "        self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "        \n",
    "    def train(self, n_iter):\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. \n",
    "        Can be called multiple times for further training.\n",
    "        \"\"\"\n",
    "        while self.curr_iter <= n_iter:\n",
    "            if self.curr_iter % 10 == 0 and n_iter > 10 and self._v:\n",
    "                print(f'\\tStill running...')\n",
    "                print(f'\\tCurrent iteration: {self.curr_iter}')\n",
    "                \n",
    "            self.training_indices = np.arange(self.n_samples)\n",
    "            np.random.shuffle(self.training_indices)\n",
    "            self.perform_sgd()\n",
    "            self.curr_iter += 1\n",
    "            \n",
    "    def predict(self, user, item):\n",
    "        \"\"\"\n",
    "        Single user and item prediction\n",
    "        \"\"\"\n",
    "        biases = self.global_bias + self.user_bias[user] + self.item_bias[item]\n",
    "        prediction_value = biases\n",
    "\n",
    "        if not self.is_bias_only: # bias-only model\n",
    "            prediction_value += self.user_vecs[user, :].dot(self.item_vecs[item, :].T)\n",
    "        \n",
    "        return prediction_value\n",
    "    \n",
    "    def perform_sgd(self):\n",
    "        for idx in self.training_indices:\n",
    "            user = self.sample_row[idx]\n",
    "            item = self.sample_col[idx]\n",
    "            prediction = self.predict(user, item)\n",
    "            actual_rating = self.ratings[user, item] # get actual rating from the dataset's ratings array \n",
    "            error = actual_rating - prediction\n",
    "            \n",
    "            # Update biases\n",
    "            self.user_bias[user] += (self.learning_rate * (error - self.user_bias_reg * self.user_bias[user]))\n",
    "            self.item_bias[item] += (self.learning_rate * (error - self.item_bias_reg * self.item_bias[item]))\n",
    "            \n",
    "            # Update latent factors\n",
    "            self.user_vecs[user, :] += self.learning_rate * (error * self.item_vecs[item, :] - self.user_fact_reg * self.user_vecs[user,:])\n",
    "            self.item_vecs[item, :] += self.learning_rate * (error * self.user_vecs[user, :] - self.item_fact_reg * self.item_vecs[item,:])\n",
    "    \n",
    "    def get_prediction_matrix(self):\n",
    "        \"\"\"\n",
    "        Predict ratings for every user and item\n",
    "        \"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        \n",
    "        for user in range(self.user_vecs.shape[0]):\n",
    "            for item in range(self.item_vecs.shape[0]):\n",
    "                predictions[user, item] = self.predict(user, item)\n",
    "                \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, predictions, test, training_rmse, test_rmse, training_mrr, test_mrr):\n",
    "        original = copy.deepcopy(predictions)\n",
    "        training_rmse.append(get_rmse(predictions, self.ratings))\n",
    "        test_rmse.append(get_rmse(predictions, test))\n",
    "\n",
    "        training_mrr.append(get_mrr(predictions, self.ratings))\n",
    "        test_mrr.append(get_mrr(predictions, test))\n",
    "        \n",
    "        self.print_verbose(f'Training RMSE = {training_rmse[-1]}')\n",
    "        self.print_verbose(f'Test RMSE = {test_rmse[-1]}')\n",
    "        \n",
    "        self.print_verbose(f'Training MRR = {training_mrr[-1]}')\n",
    "        self.print_verbose(f'Test MRR = {test_mrr[-1]}')\n",
    "\n",
    "    def print_verbose(self, msg):\n",
    "        if self._v:\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitMFALS:\n",
    "    def __init__(self,\n",
    "                 ratings,\n",
    "                 k=40,\n",
    "                 item_reg=0.0,\n",
    "                 user_reg=0.0,\n",
    "                 verbose=False):\n",
    "        self.ratings = ratings\n",
    "        self.users_count = users_count\n",
    "        self.items_count = items_count        \n",
    "        self.k_factors = k\n",
    "        self.item_reg = item_reg\n",
    "        self.user_reg = user_reg\n",
    "        self._v = verbose\n",
    "        self.curr_iter = 1\n",
    "\n",
    "    def calculate_learning_curve(self, iter_array, test, evaluate=True):\n",
    "        self.init_for_train()\n",
    "        training_rmse, test_rmse, training_mrr, test_mrr = [], [], [], []\n",
    "        \n",
    "        if not evaluate:\n",
    "            predictions_list = [None]*len(iter_array)\n",
    "        \n",
    "        for idx, n_iter in enumerate(iter_array):\n",
    "            self.print_verbose(f'k={self.k_factors}, '\\\n",
    "                               f'iterations={n_iter}, '\\\n",
    "                               f'item_reg={self.item_reg}, '\\\n",
    "                               f'user_reg={self.user_reg}')\n",
    "            self.train(n_iter)\n",
    "            predictions = self.predict_all()\n",
    "            \n",
    "            if not evaluate:\n",
    "                predictions_list[idx] = predictions\n",
    "            else:\n",
    "                self.evaluate(predictions, test, training_rmse, test_rmse, training_mrr, test_mrr)  \n",
    "        \n",
    "        print('============================================')\n",
    "        \n",
    "        return predictions_list, training_rmse, test_rmse, training_mrr, test_mrr\n",
    "    \n",
    "    def als_step_user(self):\n",
    "        YTY = self.item_vecs.T.dot(self.item_vecs)\n",
    "        lambdaI = np.eye(YTY.shape[0]) * self.user_reg\n",
    "\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            self.user_vecs[u, :] = solve((YTY + lambdaI), self.ratings[u, :].dot(self.item_vecs))\n",
    "\n",
    "    def als_step_item(self):\n",
    "        XTX = self.user_vecs.T.dot(self.user_vecs)\n",
    "        lambdaI = np.eye(XTX.shape[0]) * self.item_reg\n",
    "\n",
    "        for i in range(self.item_vecs.shape[0]):\n",
    "            self.item_vecs[i, :] = solve((XTX + lambdaI), self.ratings[:, i].T.dot(self.user_vecs))\n",
    "\n",
    "    def init_for_train(self):\n",
    "        # initialize latent vectors\n",
    "        self.user_vecs = np.random.normal(scale=1. / self.k_factors,\n",
    "                                          size=(self.users_count, self.k_factors))\n",
    "        self.item_vecs = np.random.normal(scale=1. / self.k_factors,\n",
    "                                          size=(self.items_count, self.k_factors))\n",
    "        print(f'user_vecs: {self.user_vecs.shape}, item_vecs: {self.item_vecs.shape}')\n",
    "\n",
    "    def train(self, n_iter):\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. \n",
    "        Can be called multiple times for further training.\n",
    "        \"\"\"\n",
    "        while self.curr_iter <= n_iter:\n",
    "            if self.curr_iter % 10 == 0 and n_iter > 10 and self._v:\n",
    "                print(f'\\tStill running...')\n",
    "                print(f'\\tCurrent iteration: {self.curr_iter}')\n",
    "                \n",
    "            self.als_step_user()\n",
    "            self.als_step_item()\n",
    "            self.curr_iter += 1\n",
    "            \n",
    "    def predict(self, u, i):\n",
    "        return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "\n",
    "    def predict_all(self):\n",
    "        predictions = np.zeros((self.user_vecs.shape[0],\n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "        return predictions\n",
    "                       \n",
    "    def evaluate(self, predictions, test, training_rmse, test_rmse, training_mrr, test_mrr):\n",
    "        training_rmse.append(get_rmse(predictions, self.ratings))\n",
    "        test_rmse.append(get_rmse(predictions, test))\n",
    "\n",
    "        training_mrr.append(get_mrr(predictions, self.ratings))\n",
    "        test_mrr.append(get_mrr(predictions, test))\n",
    "        \n",
    "        self.print_verbose(f'Training RMSE = {training_rmse[-1]}')\n",
    "        self.print_verbose(f'Test RMSE = {test_rmse[-1]}')\n",
    "        \n",
    "        self.print_verbose(f'Training MRR = {training_mrr[-1]}')\n",
    "        self.print_verbose(f'Test MRR = {test_mrr[-1]}')\n",
    "        \n",
    "    def print_verbose(self, msg):\n",
    "        if self._v:\n",
    "            print(msg)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from itertools import combinations, combinations_with_replacement, permutations\n",
    "\n",
    "def permutation_regularization_generator(base_dict):\n",
    "#     regularization_parameters_values = [0.01, 0.1, 1.0]\n",
    "    regularization_parameters_values = [0.01]\n",
    "    regularization_permutations = set()\n",
    "    number_of_regularization_params = len(base_dict)\n",
    "    \n",
    "    for seq in list(combinations_with_replacement(regularization_parameters_values, number_of_regularization_params)):\n",
    "        for perm_seq in permutations(seq, number_of_regularization_params):\n",
    "            regularization_permutations.add(perm_seq)\n",
    "\n",
    "    regularization_parameters_dict_list = []      \n",
    "    keys = base_dict.keys()\n",
    "    \n",
    "    for perm in regularization_permutations:\n",
    "        curr_values_dict = copy.deepcopy(base_dict)\n",
    "        \n",
    "        for idx, key in enumerate(keys):\n",
    "            curr_values_dict[key] = perm[idx]\n",
    "        \n",
    "        regularization_parameters_dict_list.append(curr_values_dict)\n",
    "        \n",
    "    return regularization_parameters_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list_of_regularization_parameters_values(model_type):\n",
    "    if model_type.lower() == \"als\":\n",
    "        base_dict = {'item_reg': 0.0, 'user_reg':0.0}\n",
    "    else: # model type == \"sgd\"\n",
    "        base_dict = {'item_fact_reg': 0.0, 'user_fact_reg': 0.0, 'item_bias_reg': 0.0, 'user_bias_reg':0.0}\n",
    "        \n",
    "    return permutation_regularization_generator(base_dict)        \n",
    "\n",
    "    \n",
    "def als_model_provider(**params):\n",
    "    return ExplicitMFALS(**params)\n",
    "\n",
    "\n",
    "def sgd_model_provider(**params):\n",
    "    return ExplicitMFSGD(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:10, b:20, c:35, d:45\n"
     ]
    }
   ],
   "source": [
    "# CODE EXAMPLE ON HOW I PASSED PARAMS:\n",
    "\n",
    "def y(a=0, b=1, c=2, d=3):\n",
    "    print(f'a:{a}, b:{b}, c:{c}, d:{d}')\n",
    "    \n",
    "def x(**params):\n",
    "    y(**params)\n",
    "    \n",
    "last_params = {'c': 35, 'd': 45}\n",
    "x(a=10,b=20,**last_params)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_learning_curve(training_data, test_data, model_provider, model_type=\"sgd\"):\n",
    "#     regularization_parameters_values = generate_list_of_regularization_parameters_values(model_type)    \n",
    "#     perm_count = 1\n",
    "#     params_and_results = []\n",
    "    \n",
    "#     if model_type.lower() == \"als\":\n",
    "#         hyper_param_learning_iter_num = len(k_options)*len(regularization_parameters_values)\n",
    "#     else: # sgd\n",
    "#         hyper_param_learning_iter_num = len(k_options)*len(learning_rate_options)*len(regularization_parameters_values)\n",
    "        \n",
    "#     for k in k_options:\n",
    "#         if model_type.lower() == \"als\":\n",
    "#             for regularization_parameters_value in regularization_parameters_values:    \n",
    "#                 print(f'iteration {perm_count}/{hyper_param_learning_iter_num}')\n",
    "#                 model = model_provider(ratings=training_data, k_factors=k, verbose=True, **regularization_parameters_value)\n",
    "#                 training_rmse, test_rmse, training_mrr, test_mrr = model.calculate_learning_curve(iter_array, test_data)\n",
    "#                 curr_run_params_and_results = {\"k\": k, \"training_rmse\": training_rmse, \"test_rmse\": test_rmse, \"training_mrr\": training_mrr, \"test_mrr\": test_mrr}\n",
    "#                 params_and_results.append({**curr_run_params_and_results, **regularization_parameters_value})    \n",
    "#                 perm_count += 1\n",
    "#         else: # sgd\n",
    "#             for learning_rate in learning_rate_options:\n",
    "#                 for regularization_parameters_value in regularization_parameters_values:    \n",
    "#                     print(f'iteration {perm_count}/{hyper_param_learning_iter_num} ')\n",
    "#                     model = model_provider(ratings=training_data, k_factors=k, verbose=True, is_bias_only=False, **regularization_parameters_value)\n",
    "#                     training_rmse, test_rmse, training_mrr, test_mrrיא = model.calculate_learning_curve(iter_array, test_data, learning_rate)\n",
    "#                     curr_run_params_and_results = {\"k\": k, \"learning_rate\": learning_rate, \"training_rmse\": training_rmse, \"test_rmse\": test_rmse, \"training_mrr\": training_mrr, \"test_mrr\": test_mrr}\n",
    "#                     params_and_results.append({**curr_run_params_and_results, **regularization_parameters_value})    \n",
    "#                     perm_count += 1                \n",
    "#     return params_and_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_model(model_provider, params_and_results, model_ctor_input, model_input):\n",
    "    model = model_provider(**model_ctor_input)\n",
    "    predictions_list, training_rmse, test_rmse, training_mrr, test_mrr = model.calculate_learning_curve(**model_input)    \n",
    "    curr_run_params_and_results = {**model_input, \"predictions_list\": predictions_list, \"training_rmse\": training_rmse, \"test_rmse\": test_rmse, \"training_mrr\": training_mrr, \"test_mrr\": test_mrr}    \n",
    "    params_and_results.append(curr_run_params_and_results)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_learning_curve(training_data, test_data, model_provider, model_type=\"sgd\", verbose=True, evaluate=True, pickle_file_name=None):\n",
    "    regularization_parameters_values = generate_list_of_regularization_parameters_values(model_type)    \n",
    "    perm_count = 1\n",
    "    params_and_results = []\n",
    "    \n",
    "    model_ctor_input = {\"ratings\": training_data, \"verbose\": verbose}        \n",
    "    model_input = {\"iter_array\": iter_array, \"test\": test_data, \"evaluate\": evaluate}\n",
    "    \n",
    "    hyper_param_learning_iter_num = len(k_options)*len(regularization_parameters_values)\n",
    "    if model_type.lower() == \"sgd\":\n",
    "        hyper_param_learning_iter_num *= len(learning_rate_options)\n",
    "        \n",
    "    if pickle_file_name and not evaluate:\n",
    "        pickle_file = open(pickle_file_name, 'ab+')\n",
    "    for k in k_options:\n",
    "        model_ctor_input[\"k\"] = k\n",
    "        for regularization_parameters_value in regularization_parameters_values:\n",
    "            model_ctor_input = {**model_ctor_input, **regularization_parameters_value}\n",
    "            if model_type.lower() == \"als\":\n",
    "                print(f'iteration {perm_count}/{hyper_param_learning_iter_num}')\n",
    "                execute_model(model_provider, params_and_results, model_ctor_input, model_input)  \n",
    "                if pickle_file_name and not evaluate:\n",
    "                    pickle.dump(params_and_results[-1], pickle_file)\n",
    "                perm_count += 1\n",
    "            else: # sgd\n",
    "                for learning_rate in learning_rate_options:\n",
    "                    print(f'iteration {perm_count}/{hyper_param_learning_iter_num}')\n",
    "                    model_ctor_input[\"is_bias_only\"] = False\n",
    "                    model_input[\"learning_rate\"] = learning_rate\n",
    "                    execute_model(model_provider, params_and_results, model_ctor_input, model_input)  \n",
    "                    if pickle_file_name and not evaluate:\n",
    "                        pickle.dump(params_and_results[-1], pickle_file)                    \n",
    "                    perm_count += 1\n",
    "                    \n",
    "    if pickle_file_name and not evaluate:    \n",
    "        pickle_file.close()\n",
    "    return params_and_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions_list, test):\n",
    "    training_rmse, test_rmse, training_mrr, test_mrr = [], [], [], []   \n",
    "    \n",
    "    for idx, predictions in enumerate(predictions_list):\n",
    "        print(idx)\n",
    "        training_rmse.append(get_rmse(predictions, self.ratings))\n",
    "        test_rmse.append(get_rmse(predictions, test))\n",
    "\n",
    "        training_mrr.append(get_mrr(predictions, self.ratings))\n",
    "        test_mrr.append(get_mrr(predictions, test))\n",
    "\n",
    "        self.print_verbose(f'Training RMSE = {training_rmse[-1]}')\n",
    "        self.print_verbose(f'Test RMSE = {test_rmse[-1]}')\n",
    "\n",
    "        self.print_verbose(f'Training MRR = {training_mrr[-1]}')\n",
    "        self.print_verbose(f'Test MRR = {test_mrr[-1]}')\n",
    "    return training_rmse, test_rmse, training_mrr, test_mrr \n",
    "\n",
    "\n",
    "def evaluate_als(params_and_results, predictions_list, test):\n",
    "    for single_params_and_results in params_and_results:\n",
    "        predictions_list = single_params_and_results[\"predictions_list\"]\n",
    "        k, item_reg, user_reg = single_params_and_results['k'], single_params_and_results['item_reg'], single_params_and_results['user_reg']\n",
    "        print(f'Predictions for k:{k}, item_reg:{item_reg}, user_reg: {user_reg}')\n",
    "\n",
    "        evalute(predictions_list, test)\n",
    "            \n",
    "            \n",
    "def evaluate_sgd(predictions_list, test):\n",
    "    for single_params_and_results in params_and_results:\n",
    "        predictions_list = single_params_and_results[\"predictions_list\"]\n",
    "        k, item_fact_reg, user_fact_reg, item_bias_reg, user_bias_reg = single_params_and_results['k'], single_params_and_results['item_fact_reg'], single_params_and_results['user_fact_reg'], single_params_and_results['item_bias_reg'], single_params_and_results['user_bias_reg']\n",
    "        print(f'Predictions for k:{k}, item_fact_reg:{item_fact_reg}, user_fact_reg: {user_fact_reg}, item_bias_reg: {item_bias_reg}, user_bias_reg: {user_bias_reg}')\n",
    "        evalute(predictions_list, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OFFICIAL AND FINAL PARAMETERS FOR SUBMISSION\n",
    "k_options = [5, 10, 20]\n",
    "learning_rate_options = [0.001, 0.01, 0.1]\n",
    "iter_array = [1, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# DEMO AND SMALLER PARAMETERS - NOT FOR SUBMISSION\n",
    "k_options = [5, 10]\n",
    "learning_rate_options = [0.1]\n",
    "iter_array = [1, 10, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_permutations_string(model_type=\"sgd\"):\n",
    "    regularization_parameters_values = generate_list_of_regularization_parameters_values(model_type)\n",
    "    print(f'Total of {len(regularization_parameters_values)} regularization parameters permutations')\n",
    "\n",
    "    if model_type.lower() == \"als\":\n",
    "        permutations_string = \"(|k's| * |reg's|^2)\"\n",
    "        hyper_param_learning_iter_num = len(k_options)*len(regularization_parameters_values)\n",
    "    else: # sgd\n",
    "        permutations_string = \"(|k's| * |alphas| * |reg's|^4)\"\n",
    "        hyper_param_learning_iter_num = len(k_options)*len(learning_rate_options)*len(regularization_parameters_values)\n",
    "\n",
    "    print(f'For each fold - {hyper_param_learning_iter_num} permutations (runs) on the model parameters {permutations_string}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test/Collect ALS 5-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 1 regularization parameters permutations\n",
      "For each fold - 2 permutations (runs) on the model parameters (|k's| * |reg's|^2)\n",
      "============================================\n",
      "Running on fold 0/1\n",
      "iteration 1/2\n",
      "user_vecs: (943, 5), item_vecs: (1682, 5)\n",
      "============================================\n",
      "iteration 2/2\n",
      "user_vecs: (943, 10), item_vecs: (1682, 10)\n",
      "============================================\n",
      "============================================\n",
      "Running on fold 1/1\n",
      "iteration 1/2\n",
      "user_vecs: (943, 5), item_vecs: (1682, 5)\n",
      "============================================\n",
      "iteration 2/2\n",
      "user_vecs: (943, 10), item_vecs: (1682, 10)\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "als_folds_results = []\n",
    "\n",
    "get_number_of_permutations_string(model_type=\"als\")\n",
    "    \n",
    "for idx, fold in enumerate(folds, start=1):\n",
    "    pickle_file_name = f\"fold{idx}_als_intermediate_results.pkl\"    \n",
    "    print('============================================')\n",
    "    print(f'Running on fold {idx}/{len(folds)}')\n",
    "    training_data, test_data = fold\n",
    "    params_and_results = calculate_learning_curve(training_data, test_data, model_provider=als_model_provider, model_type=\"als\", verbose=False, evaluate=False, pickle_file_name=pickle_file_name)\n",
    "    als_folds_results.append(params_and_results)\n",
    "    \n",
    "    pickle_file_name = f\"fold{idx}_als_full_results.pkl\"\n",
    "    with open(pickle_file_name, 'wb') as fp:\n",
    "        pickle.dump(params_and_results, fp)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test/Collect SGD 5-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 1 regularization parameters permutations\n",
      "For each fold - 2 permutations (runs) on the model parameters (|k's| * |alphas| * |reg's|^4)\n",
      "============================================\n",
      "Running on fold 0/1\n",
      "iteration 1/2\n",
      "k=5, alpha=0.1, iterations=1, item_fact_reg=0.01, user_fact_reg=0.01, item_bias_reg=0.01, user_bias_reg=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-81-fc4889ace07a>:148: RuntimeWarning: overflow encountered in multiply\n",
      "  self.item_vecs[item, :] += self.learning_rate * (error * self.user_vecs[user, :] - self.item_fact_reg * self.item_vecs[item,:])\n",
      "<ipython-input-81-fc4889ace07a>:147: RuntimeWarning: overflow encountered in multiply\n",
      "  self.user_vecs[user, :] += self.learning_rate * (error * self.item_vecs[item, :] - self.user_fact_reg * self.user_vecs[user,:])\n",
      "<ipython-input-81-fc4889ace07a>:148: RuntimeWarning: invalid value encountered in subtract\n",
      "  self.item_vecs[item, :] += self.learning_rate * (error * self.user_vecs[user, :] - self.item_fact_reg * self.item_vecs[item,:])\n",
      "<ipython-input-81-fc4889ace07a>:147: RuntimeWarning: invalid value encountered in add\n",
      "  self.user_vecs[user, :] += self.learning_rate * (error * self.item_vecs[item, :] - self.user_fact_reg * self.user_vecs[user,:])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-27a54a195189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Running on fold {idx}/{len(folds) - 1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mparams_and_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd_model_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sgd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msgd_folds_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_and_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-82788aba0caf>\u001b[0m in \u001b[0;36mcalculate_learning_curve\u001b[0;34m(training_data, test_data, model_provider, model_type)\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mmodel_ctor_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is_bias_only\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mmodel_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     \u001b[0mexecute_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_and_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ctor_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                     \u001b[0mperm_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams_and_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-e13f719ac58c>\u001b[0m in \u001b[0;36mexecute_model\u001b[0;34m(model_provider, params_and_results, model_ctor_input, model_input)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexecute_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_and_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ctor_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_ctor_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtraining_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_mrr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcurr_run_params_and_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_rmse\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_rmse\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_mrr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_mrr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_mrr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_mrr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mparams_and_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_run_params_and_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-fc4889ace07a>\u001b[0m in \u001b[0;36mcalculate_learning_curve\u001b[0;34m(self, iter_array, test, learning_rate)\u001b[0m\n\u001b[1;32m     68\u001b[0m                               f'user_bias_reg={self.user_bias_reg}')\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prediction_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;31m#             try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_mrr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mrr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-fc4889ace07a>\u001b[0m in \u001b[0;36mget_prediction_matrix\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-fc4889ace07a>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, user, item)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bias_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# bias-only model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mprediction_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbiases\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprediction_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sgd_folds_results = []\n",
    "\n",
    "get_number_of_permutations_string(model_type=\"sgd\")\n",
    "\n",
    "for idx, fold in enumerate(folds, start=1):\n",
    "    pickle_file_name = f\"fold{idx}_sgd_intermediate_results.pkl\"    \n",
    "    print('============================================')\n",
    "    print(f'Running on fold {idx}/{len(folds)}')\n",
    "    training_data, test_data = fold\n",
    "    params_and_results = calculate_learning_curve(training_data, test_data, model_provider=sgd_model_provider, model_type=\"sgd\", verbose=False, evaluate=False, pickle_file_name=pickle_file_name)\n",
    "    sgd_folds_results.append(params_and_results)\n",
    "    \n",
    "    pickle_file_name = f\"fold{idx}_sgd_full_results.pkl\"\n",
    "    with open(pickle_file_name, 'wb') as fp:\n",
    "        pickle.dump(params_and_results, fp)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def generate_key(dict_item, model_type=\"sgd\"):\n",
    "    if model_type.lower() == \"als\":\n",
    "        key = f\"k_{dict_item['k']}_ir_{dict_item['item_reg']}_ur_{dict_item['user_reg']}\"\n",
    "    else:\n",
    "        key = f\"k_{dict_item['k']}_lr_{dict_item['learning_rate']}_if_{dict_item['item_fact_reg']}_uf_{dict_item['user_fact_reg']}_ib_{dict_item['item_bias_reg']}_ub_{dict_item['user_bias_reg']}\"\n",
    "    \n",
    "    return key\n",
    "\n",
    "\n",
    "def calculate_average(matrix):\n",
    "    '''\n",
    "    Take the mean of each column from a given matrix\n",
    "    '''\n",
    "    return np.mean(matrix, axis=0)\n",
    "\n",
    "\n",
    "def get_average_map(given_map):\n",
    "    for key, value in given_map.items():\n",
    "        matrix = np.array(value)\n",
    "        new_value = calculate_average(matrix)\n",
    "        given_map[key] = new_value\n",
    "        \n",
    "    return given_map\n",
    "\n",
    "\n",
    "def create_error_map(folds_results_list, key_name, model_type=\"sgd\"):\n",
    "    results_map = defaultdict(list)\n",
    "\n",
    "    for fold_result in folds_results_list:\n",
    "        for dict_item in fold_result:\n",
    "            generated_key = generate_key(dict_item, model_type)\n",
    "            current_error_list = dict_item[key_name]\n",
    "            results_map[generated_key].append(current_error_list)\n",
    "            \n",
    "    results_map = get_average_map(results_map)\n",
    "    \n",
    "    return results_map\n",
    "\n",
    "\n",
    "def combine_training_and_test_averages_per_model(folds_results_list, averages_training_map, averages_test_map, key_names, model_type=\"sgd\"):\n",
    "    averaged_folds_results = []\n",
    "    \n",
    "    for fold_result in folds_results_list[0]:\n",
    "        dict_copy = fold_result.copy()\n",
    "        \n",
    "        dict_copy[key_names[0]] = averages_training_map[generate_key(fold_result, model_type)]\n",
    "        dict_copy[key_names[1]] = averages_test_map[generate_key(fold_result, model_type)]\n",
    "        \n",
    "        averaged_folds_results.append(dict_copy)\n",
    "            \n",
    "    return averaged_folds_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo = [\n",
    "#     [\n",
    "#     {'k': 10,\n",
    "#    'learning_rate': 0.001,\n",
    "#    'training_rmse': [0,0,0],\n",
    "#    'test_rmse': [1,1,1],\n",
    "#    'item_fact_reg': 100.0,\n",
    "#    'user_fact_reg': 100.0,\n",
    "#    'item_bias_reg': 100.0,\n",
    "#    'user_bias_reg': 100.0},\n",
    "#        {'k': 20,\n",
    "#    'learning_rate': 0.001,\n",
    "#    'training_rmse': [1,2,3],\n",
    "#    'test_rmse': [0,0,0],\n",
    "#    'item_fact_reg': 100.0,\n",
    "#    'user_fact_reg': 100.0,\n",
    "#    'item_bias_reg': 100.0,\n",
    "#    'user_bias_reg': 100.0}\n",
    "#        ],\n",
    "#     [\n",
    "#     {'k': 10,\n",
    "#    'learning_rate': 0.001,\n",
    "#    'training_rmse': [1,1,1],\n",
    "#    'test_rmse': [2,2,2],\n",
    "#    'item_fact_reg': 100.0,\n",
    "#    'user_fact_reg': 100.0,\n",
    "#    'item_bias_reg': 100.0,\n",
    "#    'user_bias_reg': 100.0},\n",
    "#        {'k': 20,\n",
    "#    'learning_rate': 0.001,\n",
    "#    'training_rmse': [5,5,5],\n",
    "#    'test_rmse': [5,5,5],\n",
    "#    'item_fact_reg': 100.0,\n",
    "#    'user_fact_reg': 100.0,\n",
    "#    'item_bias_reg': 100.0,\n",
    "#    'user_bias_reg': 100.0}\n",
    "#        ],\n",
    "# ]\n",
    "\n",
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============= RMSE\n",
    "training_sgd_averages_map_rmse = create_error_map(sgd_folds_results, 'training_rmse', model_type=\"sgd\")\n",
    "test_sgd_averages_map_rmse = create_error_map(sgd_folds_results, 'test_rmse', model_type=\"sgd\")\n",
    "\n",
    "training_als_averages_map_rmse = create_error_map(als_folds_results, 'training_rmse', model_type=\"als\")\n",
    "test_als_averages_map_rmse = create_error_map(als_folds_results, 'test_rmse', model_type=\"als\")\n",
    "\n",
    "\n",
    "# ============= MRR\n",
    "\n",
    "# training_sgd_mrr\n",
    "# test_sgd_mrr\n",
    "training_als_averages_map_mrr = create_error_map(als_folds_results, 'training_mrr', model_type=\"als\")\n",
    "test_als_averages_map_mrr = create_error_map(als_folds_results, 'test_mrr', model_type=\"als\")\n",
    "\n",
    "# ============= nDCG\n",
    "\n",
    "\n",
    "print(training_sgd_averages_map_rmse)\n",
    "print(test_sgd_averages_map_rmse)\n",
    "print()\n",
    "print(training_als_averages_map_rmse)\n",
    "print(test_als_averages_map_rmse)\n",
    "print()\n",
    "print(training_als_averages_map_mrr)\n",
    "print(test_als_averages_map_mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_names_array=['training_rmse', 'test_rmse', 'training_mrr', 'test_mrr']\n",
    "sgd_averages_result_rmse = combine_training_and_test_averages_per_model(sgd_folds_results, training_sgd_averages_map_rmse, test_sgd_averages_map_rmse, key_names=['training_rmse', 'test_rmse'], model_type=\"sgd\")\n",
    "print(sgd_averages_result_rmse)\n",
    "print()\n",
    "\n",
    "als_averages_result = combine_training_and_test_averages_per_model(als_folds_results, training_als_averages_map_rmse, test_als_averages_map_rmse, key_names=key_names_array, model_type=\"als\")\n",
    "print(als_averages_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "def plot_learning_curve_list(iter_array, predictions, title, error_method, mse_extractor, model_type=\"als\"):\n",
    "    plt.figure(num=None, figsize=(20, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for prediction in predictions:\n",
    "        k = prediction[\"k\"]\n",
    "        \n",
    "        if model_type.lower() == \"als\":\n",
    "            ir, ur = prediction['item_reg'], prediction['user_reg']\n",
    "            plt.plot(iter_array, mse_extractor(prediction), label=f\"{title} k={k}, item_reg={ir}, user_reg={ur}\", linewidth=5)\n",
    "        else:\n",
    "            learning_rate = prediction[\"learning_rate\"]\n",
    "            if_reg, uf_reg, ib, ub = prediction['item_fact_reg'], prediction['user_fact_reg'], prediction['item_bias_reg'], prediction['user_bias_reg']\n",
    "            plt.plot(iter_array, mse_extractor(prediction), label=f\"{title} k={k}, α={learning_rate}, if={if_reg}, uf={uf_reg}, if={if_reg}, uf={uf_reg}, ib={ib}, ub={ub}\", linewidth=5)\n",
    "    \n",
    "    plt.xticks(fontsize=16);\n",
    "    plt.yticks(fontsize=16);\n",
    "    plt.xlabel('iterations', fontsize=20);\n",
    "    plt.ylabel(f'{error_method} (averaged)', fontsize=20);\n",
    "    plt.title(f'{title} - {model_type.upper()} - ({error_method})', fontsize=30, bbox={'facecolor':'k', 'pad':5}, color='w')\n",
    "    plt.legend(loc='best', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_TRAINING = 'TRAINING'\n",
    "TITLE_TEST = 'TEST'\n",
    "ERROR_METHOD_RMSE = 'RMSE'\n",
    "ERROR_METHOD_MRR = 'MRR'\n",
    "ERROR_METHOD_NDCG = 'NDCG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve_list(iter_array, title=TITLE_TRAINING, error_method = ERROR_METHOD_RMSE, mse_extractor=lambda model: model[\"training_rmse\"], model_type=\"sgd\", predictions=sgd_averages_result_rmse)\n",
    "plot_learning_curve_list(iter_array, title=TITLE_TEST, error_method = ERROR_METHOD_RMSE, mse_extractor=lambda model: model[\"test_rmse\"], model_type=\"sgd\", predictions=sgd_averages_result_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve_list(iter_array, title=TITLE_TRAINING, error_method=ERROR_METHOD_RMSE, mse_extractor=lambda model: model[\"training_rmse\"], model_type=\"als\", predictions=als_averages_result)\n",
    "plot_learning_curve_list(iter_array, title=TITLE_TEST, error_method=ERROR_METHOD_RMSE, mse_extractor=lambda model: model[\"test_rmse\"], model_type=\"als\", predictions=als_averages_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve_list(iter_array, title=TITLE_TRAINING, error_method=ERROR_METHOD_MRR, mse_extractor=lambda model: model[\"training_mrr\"], model_type=\"als\", predictions=als_averages_result)\n",
    "plot_learning_curve_list(iter_array, title=TITLE_TEST, error_method=ERROR_METHOD_MRR, mse_extractor=lambda model: model[\"test_mrr\"], model_type=\"als\", predictions=als_averages_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Matrix factorization – item similarity and model explainability (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
