{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations Systems\n",
    "## Course Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to let you practice in a data scientist daily work by leveraging recommender\n",
    "systems algorithms you learnt in the course and customize them in order to solve real business\n",
    "problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset based on the <a href='https://grouplens.org/datasets/movielens/1m/'>MovieLens 1M rating dataset</a> after some pre-processing to adapt it to an implicit feedback use case scenario.  \n",
    "You can download the dataset used by <a href='https://github.com/hexiangnan/neural_collaborative_filtering'>this implementation</a> of the paper Neural Collaborative Filtering or from the NeuralCollaborativeFiltering_implicit notebook in Moodle.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-e85a59d2f385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Deep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import os  \n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import multiprocessing\n",
    "from icecream import ic\n",
    "\n",
    "# Data Science\n",
    "import numpy  as np\n",
    "import scipy  as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "# Notebook\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Deep\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import Dense,Lambda,Activation\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop,Adamax\n",
    "from tensorflow.keras.models import Sequential,Model,load_model,save_model\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Embedding,Input,Dense,Reshape,Flatten,Dropout,Multiply,Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5 # used to evaluate the model. Top K recommendations are used.\n",
    "num_factors = 8 # size of embedding size. Can be split to 4 different params potentially.\n",
    "num_negatives = 4 # how many negative samples per positive sample?\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "verbose = 1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "\n",
    "# Read the training file\n",
    "train = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.train.rating', sep='\\t', names=column_names)\n",
    "\n",
    "# Read the test file\n",
    "test_rating = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.test.rating', sep='\\t', names=column_names)\n",
    "\n",
    "\n",
    "negative_ids = ['(user_id, item_id)']\n",
    "\n",
    "for i in range(1,100):\n",
    "    negative_ids.append(f'id-{i}')\n",
    "\n",
    "test_negative = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.test.negative', sep='\\t', names=negative_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>978824330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>978824330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0        0       32       4  978824330\n",
       "1        0       34       4  978824330\n",
       "2        0        4       5  978824291\n",
       "3        0       35       4  978824291\n",
       "4        0       30       4  978824291"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(994169, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "znCqCTMtjmaW",
    "outputId": "478c6850-880c-4a9b-b2dd-6638ef7962b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994169 users\n",
      "3704 items\n"
     ]
    }
   ],
   "source": [
    "num_users = train.shape[0]\n",
    "num_items = len(train.item_id.unique())\n",
    "\n",
    "print(str(num_users) + ' users')\n",
    "print(str(num_items) + ' items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Matrix Factorization with custom loss (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) Answer:\n",
    "\n",
    "Given\n",
    "$$\n",
    "L=-\\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(y_{i, j} \\log \\left(\\sigma\\left(\\mu+p_{i}+o_{j}+\\boldsymbol{u}_{i}^{T} v_{j}\\right)\\right)+\\left(1-y_{i, j}\\right) \\log \\left(1-\\sigma\\left(\\mu+p_{i}+o_{j}+\\boldsymbol{u}_{i}^{T} v_{j}\\right)\\right)\\right)\n",
    "$$\n",
    "\n",
    "Note that:\n",
    "$$\n",
    "\\log (\\sigma (z)) \\newline\n",
    "= \\log (\\frac{1}{1+e^{-z}}) \\newline\n",
    "= \\log(1) - \\log(1+e^{-z}) \\newline\n",
    "= -\\log(1+e^{-z})\n",
    "$$\n",
    "* (Here, we used the fact that $\\log(\\frac{x}{y}) = \\log(x) - \\log(y)$)\n",
    "\n",
    "Similarly:\n",
    "$$\n",
    "\\log (1 - \\sigma (z)) \\newline \\newline\n",
    "= \\log (1 - \\frac{1}{1+e^{-z}}) \\newline\n",
    "= \\log (\\frac{1+e^{-z} - 1}{1+e^{-z}}) \\newline\n",
    "= \\log (\\frac{e^{-z}}{1+e^{-z}}) \\newline\n",
    "= \\log(e^{-z}) - \\log(1+e^{-z}) \\newline\n",
    "= -z - \\log(1+e^{-z}) \\newline\n",
    "= -\\big( z + \\log(1+e^{-z}) \\big)\n",
    "$$\n",
    "\n",
    "\n",
    "**Denote** $\\boldsymbol{z = \\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j}}$ <br>\n",
    "Now, using $z$ and plugging in the two simplified expressions from above, we obtain:\n",
    "$$\n",
    "L=-\\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(y_{i, j} \\log \\left(\\sigma\\left(z\\right)\\right)+\\left(1-y_{i, j}\\right) \\log \\left(1-\\sigma\\left(z\\right)\\right)\\right) \\newline\n",
    "=-\\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(-1 \\cdot y_{i, j} \\log(1+e^{-z}) +\\left(1-y_{i, j}\\right) \n",
    "\\left( -(z + \\log(1+e^{-z}))\\right)\\right) \\newline\n",
    "=-\\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(-1 \\cdot y_{i, j} \\log(1+e^{-z}) - \\left(1-y_{i, j}\\right)(z + \\log(1+e^{-z})) \\right) \\newline\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(y_{i, j} \\log(1+e^{-z}) + \\left(1-y_{i, j}\\right)(z + \\log(1+e^{-z})) \\right) \\newline\n",
    "$$\n",
    "\n",
    "After opening up the multiplications, we end up with:\n",
    "$$\n",
    "L= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left( z - y_{i, j} z + \\log(1+e^{-z}) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "- Reference: https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need now is to compute the partial derivatives of $L$ w.r.t the user and item latent vector weights ($u_i,  v_j$), the global bias ($\\mu$), user bias ($p_i$) and item bias ($o_j$) variables for user i and item j.<br>\n",
    "The update rules are:\n",
    "$$\n",
    "u = u - \\frac{\\partial L}{\\partial u} \\newline\n",
    "v = v - \\frac{\\partial L}{\\partial v} \\newline\n",
    "\\mu = \\mu - \\frac{\\partial L}{\\partial \\mu} \\newline\n",
    "p = p - \\frac{\\partial L}{\\partial p} \\newline\n",
    "o = o - \\frac{\\partial L}{\\partial o}\n",
    "$$\n",
    "\n",
    "Using the \"Chain Rule\" - we solve each derivative separately and then plug back in:\n",
    "\n",
    "\n",
    "- **user latent vector weights ($u_i$):**\n",
    "$$\n",
    "\\frac{\\partial [\\alpha_{j} z]}{\\partial u_i}\n",
    "= \\frac{\\partial [\\alpha_{j} (\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j})]}{\\partial u_i} \n",
    "= \\alpha_{j} v_{j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial [-\\alpha_{j}  y_{i, j} z]}{\\partial u_i}\n",
    "= \\frac{\\partial [- \\alpha_{j} y_{i, j}(\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j})]} {\\partial u_i} \n",
    "= -\\alpha_{j} y_{i, j} v_{j}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial [\\alpha_{j}  \\log(1+e^{-z})]}{\\partial u_i}\n",
    "= \\frac{\\alpha_j e^{-z} \\frac{\\partial z}{\\partial u_i}} {1 + e^{-z}}\n",
    "= - \\frac{\\alpha_{j} e^{-z} v_{j}}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "* (Last transition, log derivative, follows from [here](https://www.symbolab.com/solver/step-by-step/%5Cleft(log_%7Be%7D%5Cleft(1%2Be%5E%7B-2x%7D%5Cright)%5Cright)%5E%7B'%7D))\n",
    "\n",
    "Hence,\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial u_i}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\big( \\alpha_{j} v_{j} - \\alpha_{j} y_{i, j} v_{j} - \\frac{\\alpha_{j} e^{-z} v_{j}}{1 + e^{-z}} \\big) \n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} v_{j} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$\n",
    "\n",
    "\n",
    "- **item latent vector weights ($v_j$)** is very similar, since both terms are multiplied with each other:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial v_j}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\big( \\alpha_{j} {u}_{i}^{T} - \\alpha_{j} y_{i, j} {u}_{i}^{T} - \\frac{\\alpha_{j} e^{-z} {u}_{i}^{T}}{1 + e^{-z}} \\big) \n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} {u}_{i}^{T} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$\n",
    "\n",
    "\n",
    "- **the global bias, $\\mu$:**\n",
    "$$\n",
    "\\frac{\\partial [\\alpha_{j} z]}{\\partial \\mu}\n",
    "= \\frac{\\partial [\\alpha_{j} (\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j})]}{\\partial \\mu} \n",
    "= \\alpha_{j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial [-\\alpha_{j}  y_{i, j} z]}{\\partial \\mu}\n",
    "= \\frac{\\partial [- \\alpha_{j} y_{i, j}(\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j})]} {\\partial \\mu} \n",
    "= -\\alpha_{j} y_{i, j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial [\\alpha_{j}  \\log(1+e^{-z})]}{\\partial \\mu}\n",
    "= \\frac{\\partial [\\alpha_{j} \\log(1 + e^{\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j}})]} {\\partial \\mu} \n",
    "= \\frac{\\alpha_j e^{-z} \\frac{\\partial z}{\\partial u_i}} {1 + e^{-z}}\n",
    "= - \\frac{\\alpha_{j} e^{-z}}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mu}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\big( \\alpha_{j} - \\alpha_{j} y_{i, j} - \\frac{\\alpha_{j} e^{-z}}{1 + e^{-z}} \\big) \n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$\n",
    "\n",
    "\n",
    "- **user bias $p_i$** follows the same calculation as $\\mu$ since it's added as a standalone addition:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial p_i}\n",
    "= \\frac{\\partial L}{\\partial \\mu}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$\n",
    "\n",
    "\n",
    "- **item bias $o_j$** follows the same calculation as $\\mu$ since it's added as a standalone addition:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial o_j}\n",
    "= \\frac{\\partial L}{\\partial \\mu}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) Answer:\n",
    "\n",
    "Given the prices of the different items from the catalog, we can set the weights for each training instance based on the **item price divided by the maximal price** we have.\n",
    "Meaning, we're giving each weight its relative \"significance\".\n",
    "\n",
    "This is very intuitive and straightforward - The more expensive an item is, the larger weight it's given, where weights' values range from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 2: Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A) Use the item_price.csv file to get the prices of each item. Explore the price distribution of items.\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3701</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3702</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3703</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3705</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3706 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      price\n",
       "item       \n",
       "0         4\n",
       "1         1\n",
       "2         1\n",
       "3         2\n",
       "4         2\n",
       "...     ...\n",
       "3701     25\n",
       "3702      4\n",
       "3703      1\n",
       "3704      4\n",
       "3705      4\n",
       "\n",
       "[3706 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_price = pd.read_csv('./item_price.csv')\n",
    "item_price.set_index('item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_items = item_price.shape[0]\n",
    "prices = item_price['price']\n",
    "min_price = prices.min()\n",
    "max_price = prices.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x1a89291e9a0>,\n",
       "  <matplotlib.axis.XTick at 0x1a89291e970>,\n",
       "  <matplotlib.axis.XTick at 0x1a8929144c0>,\n",
       "  <matplotlib.axis.XTick at 0x1a892c3e0a0>,\n",
       "  <matplotlib.axis.XTick at 0x1a892c3e5b0>],\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(item_price.price, align='mid', bins=50)\n",
    "plt.xticks(item_price.price.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price\n",
       "1     1326\n",
       "2      896\n",
       "4      709\n",
       "9      589\n",
       "25     186\n",
       "Name: item, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_price.groupby('price')['item'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3706.000000</td>\n",
       "      <td>3706.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1852.500000</td>\n",
       "      <td>4.291689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1069.974377</td>\n",
       "      <td>5.496992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>926.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1852.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2778.750000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              item        price\n",
       "count  3706.000000  3706.000000\n",
       "mean   1852.500000     4.291689\n",
       "std    1069.974377     5.496992\n",
       "min       0.000000     1.000000\n",
       "25%     926.250000     1.000000\n",
       "50%    1852.500000     2.000000\n",
       "75%    2778.750000     4.000000\n",
       "max    3705.000000    25.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3706 items with prices distributed from [1, 2, 4, 9, 25].<br>\n",
    "From the histogram and the data we see that most of the items are priced 2 and lower - ((1: 1326), (2: 896), (4: 709), (9: 589), (25: 186))<br>\n",
    "It doesn't look like we can deduce a behaviour of a specific distribution (like normal/uniform etc.), especially since we don't have enough data other than ID numbers and prices...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B) To evaluate the performance of the price sensitive model we add another metric Revenue@K which measures the overall revenue from the top 5 recommended hits.\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_revenue_k(rank_list, gt_item):\n",
    "    for item in rank_list:\n",
    "        if item == gt_item:\n",
    "            return _item_price[_item_price.item == item].price\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C) Suggest a metric of your own which will incorporate both the ranking of the recommended items as well as its price. Explain why this metric is suitable and demonstrate it as part of the evaluation in point e below.\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bullet B we got the top K hits and calculated the revenue for them. What I am suggesting is to make it take into considuration the NDCG. the motivation here is that the model that will win will produce the most profit calculated from the higest rated items he manage to get a hit. and got the most gain from the fact that it predicted more top rated items in the right place then it's compatitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revenue_k.mean <-- normalized price \n",
    "\n",
    "# revenue_k.mean * ndcg_revenue_k.mean (B)\n",
    "# revenue_k.mean * ndcg.mean (A)\n",
    "# revenue_k.mean + ndcg.mean\n",
    "# revenue_k.mean + ndcg_revenue_k.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D) Select one of the models presented in the Neural Collaborative Filtering paper and incorporate the movie price to the loss function as part of training\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GMF_model(num_users, num_items, latent_dim, regs=None, activation='sigmoid'):\n",
    "    '''Generalized Matrix Factorization'''\n",
    "\n",
    "    if not regs:\n",
    "        regs = [[0,0]]\n",
    "    \n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01))  #init = init_normal, \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings \n",
    "    predict_vector = Multiply()([user_latent, item_latent]) #merge([user_latent, item_latent], mode = 'mul')\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation=activation, kernel_initializer='lecun_uniform', name = 'prediction')(predict_vector)\n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-1b3a0ef8bbb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdiff\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mitem_price\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "T = [11, 12, 5, 2]\n",
    "T2 = [10, 11, 4, 1]\n",
    "diff = np.subtract(T,T2)\n",
    "\n",
    "print(diff)\n",
    "diff * item_price['price'].values[:len(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model():\n",
    "    gmf_model = get_GMF_model(num_users, num_items, num_factors, regs = [[0,0]])\n",
    "    gmf_model.compile(optimizer=Adam(lr=learning_rate), loss=my_loss_fn)\n",
    "   \n",
    "    return gmf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### E) Compare the results of the original model and the one with the customized loss across the four metrics: (20 points)\n",
    "- MRR@5\n",
    "- NDCG@5\n",
    "- Revenue@5 \n",
    "- your custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'theano'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d1d9df7ff428>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mHe\u001b[0m \u001b[0mXiangnan\u001b[0m \u001b[0met\u001b[0m \u001b[0mal\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mNeural\u001b[0m \u001b[0mCollaborative\u001b[0m \u001b[0mFiltering\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mIn\u001b[0m \u001b[0mWWW\u001b[0m \u001b[1;36m2017.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m '''\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'theano'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Keras Implementation of Generalized Matrix Factorization (GMF) recommender model in:\n",
    "He Xiangnan et al. Neural Collaborative Filtering. In WWW 2017.  \n",
    "'''\n",
    "import theano.tensor as T\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializations\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape, Merge, Flatten\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras.regularizers import l2\n",
    "from Dataset import Dataset\n",
    "from evaluate import evaluate_model\n",
    "from time import time\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "#################### Arguments ####################\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run GMF.\")\n",
    "    parser.add_argument('--path', nargs='?', default='Data/',\n",
    "                        help='Input data path.')\n",
    "    parser.add_argument('--dataset', nargs='?', default='ml-1m',\n",
    "                        help='Choose a dataset.')\n",
    "    parser.add_argument('--epochs', type=int, default=100,\n",
    "                        help='Number of epochs.')\n",
    "    parser.add_argument('--batch_size', type=int, default=256,\n",
    "                        help='Batch size.')\n",
    "    parser.add_argument('--num_factors', type=int, default=8,\n",
    "                        help='Embedding size.')\n",
    "    parser.add_argument('--regs', nargs='?', default='[0,0]',\n",
    "                        help=\"Regularization for user and item embeddings.\")\n",
    "    parser.add_argument('--num_neg', type=int, default=4,\n",
    "                        help='Number of negative instances to pair with a positive instance.')\n",
    "    parser.add_argument('--lr', type=float, default=0.001,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--learner', nargs='?', default='adam',\n",
    "                        help='Specify an optimizer: adagrad, adam, rmsprop, sgd')\n",
    "    parser.add_argument('--verbose', type=int, default=1,\n",
    "                        help='Show performance per X iterations')\n",
    "    parser.add_argument('--out', type=int, default=1,\n",
    "                        help='Whether to save the trained model.')\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def init_normal(shape, name=None):\n",
    "    return initializations.normal(shape, scale=0.01, name=name)\n",
    "\n",
    "\n",
    "def get_model(num_users, num_items, latent_dim, regs=[0,0]):\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
    "                                  init = init_normal, W_regularizer = l2(regs[0]), input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
    "                                  init = init_normal, W_regularizer = l2(regs[1]), input_length=1)   \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings \n",
    "    predict_vector = merge([user_latent, item_latent], mode = 'mul')\n",
    "    \n",
    "    # Final prediction layer\n",
    "    #prediction = Lambda(lambda x: K.sigmoid(K.sum(x)), output_shape=(1,))(predict_vector)\n",
    "    prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = 'prediction')(predict_vector)\n",
    "    \n",
    "    model = Model(input=[user_input, item_input], \n",
    "                output=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in xrange(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while train.has_key((u, j)):\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    num_factors = args.num_factors\n",
    "    regs = eval(args.regs)\n",
    "    num_negatives = args.num_neg\n",
    "    learner = args.learner\n",
    "    learning_rate = args.lr\n",
    "    epochs = args.epochs\n",
    "    batch_size = args.batch_size\n",
    "    verbose = args.verbose\n",
    "    \n",
    "    topK = 10\n",
    "    evaluation_threads = 1 #mp.cpu_count()\n",
    "    print(\"GMF arguments: %s\" %(args))\n",
    "    model_out_file = 'Pretrain/%s_GMF_%d_%d.h5' %(args.dataset, num_factors, time())\n",
    "    \n",
    "    # Loading data\n",
    "    t1 = time()\n",
    "    dataset = Dataset(args.path + args.dataset)\n",
    "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "    num_users, num_items = train.shape\n",
    "    print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
    "          %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))\n",
    "    \n",
    "    # Build model\n",
    "    model = get_model(num_users, num_items, num_factors, regs)\n",
    "    if learner.lower() == \"adagrad\": \n",
    "        model.compile(optimizer=Adagrad(lr=learning_rate), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"rmsprop\":\n",
    "        model.compile(optimizer=RMSprop(lr=learning_rate), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"adam\":\n",
    "        model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
    "    else:\n",
    "        model.compile(optimizer=SGD(lr=learning_rate), loss='binary_crossentropy')\n",
    "    #print(model.summary())\n",
    "    \n",
    "    # Init performance\n",
    "    t1 = time()\n",
    "    (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "    hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    #mf_embedding_norm = np.linalg.norm(model.get_layer('user_embedding').get_weights())+np.linalg.norm(model.get_layer('item_embedding').get_weights())\n",
    "    #p_norm = np.linalg.norm(model.get_layer('prediction').get_weights()[0])\n",
    "    print('Init: HR = %.4f, NDCG = %.4f\\t [%.1f s]' % (hr, ndcg, time()-t1))\n",
    "    \n",
    "    # Train model\n",
    "    best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
    "    for epoch in xrange(epochs):\n",
    "        t1 = time()\n",
    "        # Generate training instances\n",
    "        user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "        \n",
    "        # Training\n",
    "        hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                         np.array(labels), # labels \n",
    "                         batch_size=batch_size, nb_epoch=1, verbose=0, shuffle=True)\n",
    "        t2 = time()\n",
    "        \n",
    "        # Evaluation\n",
    "        if epoch %verbose == 0:\n",
    "            (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "            hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "            print('Iteration %d [%.1f s]: HR = %.4f, NDCG = %.4f, loss = %.4f [%.1f s]' \n",
    "                  % (epoch,  t2-t1, hr, ndcg, loss, time()-t2))\n",
    "            if hr > best_hr:\n",
    "                best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "                if args.out > 0:\n",
    "                    model.save_weights(model_out_file, overwrite=True)\n",
    "\n",
    "    print(\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %(best_iter, best_hr, best_ndcg))\n",
    "    if args.out > 0:\n",
    "        print(\"The best GMF model is saved to %s\" %(model_out_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq # for retrieval topK\n",
    "import multiprocessing\n",
    "from time import time\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_ratings, test_negatives, K):\n",
    "    \"\"\"\n",
    "    Evaluate the performance (MRR, NDCG) of top-K recommendation\n",
    "    Return: score of each test rating.\n",
    "    \"\"\"\n",
    "    mrrs, ndcgs = zip(*[eval_one_rating(model, test_ratings, test_negatives, idx, K) for idx in range(len(test_ratings))])\n",
    "    return np.array(mrrs).mean(), np.array(ndcgs).mean()\n",
    "\n",
    "\n",
    "def eval_one_rating(model, test_ratings, test_negatives, idx, K):\n",
    "    u = test_ratings.iloc[idx].user_id\n",
    "    gtItem = test_ratings.iloc[idx].item_id\n",
    "    items = test_negatives[test_negatives['(user_id, item_id)'] == f'({u},{gtItem})'].to_numpy()[0]\n",
    "    items = items[1:len(items)].astype('int32')\n",
    "    items = np.append(items, gtItem)\n",
    "    users = np.full(len(items), u, dtype = 'int32')\n",
    "    # Get prediction scores\n",
    "    map_item_score = {}\n",
    "    predictions = model.predict([users, np.array(items)], \n",
    "                                 batch_size=100, verbose=0)\n",
    "    \n",
    "    for i in range(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = predictions[i]\n",
    "    \n",
    "    items = items[:len(items)-1]\n",
    "    \n",
    "    # Evaluate top rank list\n",
    "    ranklist = heapq.nlargest(K, map_item_score, key=map_item_score.get)\n",
    "    mrr = getMRR(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    \n",
    "    return mrr, ndcg\n",
    "\n",
    "\n",
    "def getMRR(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        \n",
    "        if item == gtItem:\n",
    "            return 1/(i+1)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        \n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: empty expression not allowed (<ipython-input-21-c33bed0d6098>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-c33bed0d6098>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    print(f'{name}, k: {K}, Rev@K = {}, MRR = {mrr:.4f}, NDCG = {ndcg:.4f}\\t t1 = [{(time()-t1)/60}s]')\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m f-string: empty expression not allowed\n"
     ]
    }
   ],
   "source": [
    "def initPerformance(name, model):\n",
    "    t1 = time()\n",
    "    mrr, ndcg = evaluate_model(model, test_rating, test_negative, K)\n",
    "    print(f'{name}, k: {K}, Rev@K = {}, MRR = {mrr:.4f}, NDCG = {ndcg:.4f}\\t t1 = [{(time()-t1)/60}s]')\n",
    "    return mrr, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_model = build_model()\n",
    "mrr_GMF, ndcg_GMF = initPerformance('GMF', gmf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "M7nlDtDaCakQ"
   },
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [0]*((num_negatives + 1)*len(train)),[0]*((num_negatives + 1)*len(train)),[1]\n",
    "    num_users = train.shape[0]\n",
    "    all_items = training.item_id.unique().argsort()\n",
    "    \n",
    "    negatives = [0]*num_negatives\n",
    "    labels.extend(negatives)\n",
    "    total_labels = []\n",
    "    list(map(lambda x: total_labels.extend(labels), range(len(train))))\n",
    "#     return\n",
    "    percent_1 = int(len(train)/100)\n",
    "    ic(percent_1)\n",
    "    \n",
    "    items_the_user_didnt_rank = None\n",
    "    prev_user = -1\n",
    "    chosen_item_per_user = []\n",
    "    \n",
    "    for idx_i in range(len(train)):\n",
    "        curr_index = idx_i * (num_negatives + 1)\n",
    "        if idx_i != 0 and idx_i % percent_1 == 0:\n",
    "            print(f'{int(idx_i/percent_1)}%')\n",
    "        u = train.iloc[idx_i].user_id\n",
    "        i = train.iloc[idx_i].item_id\n",
    "\n",
    "        user_input[curr_index:curr_index + (num_negatives + 1)] = [u]*(num_negatives + 1)\n",
    "\n",
    "        item_input[curr_index] = i\n",
    "        \n",
    "        if u != prev_user:\n",
    "            items = training[training['user_id'] == u].item_id.to_numpy().argsort()\n",
    "            items_the_user_didnt_rank = all_items[~np.in1d(all_items,items)]\n",
    "            prev_user = u\n",
    "#             chosen_item_per_user = []\n",
    "\n",
    "#         items_the_user_didnt_rank = items_the_user_didnt_rank[~np.in1d(items_the_user_didnt_rank,chosen_item_per_user)]\n",
    "        sample_items = items_the_user_didnt_rank[np.random.choice(len(items_the_user_didnt_rank), size=num_negatives, replace=False)]\n",
    "        item_input[curr_index+1:curr_index + (num_negatives + 1)] = sample_items\n",
    "#         chosen_item_per_user.extend(sample_items)\n",
    "\n",
    "    return user_input, item_input, total_labels\n",
    "\n",
    "\n",
    "\n",
    "# THE ORIGINAL CODE:\n",
    "\n",
    "# def get_train_instances(train, num_negatives):\n",
    "#     user_input, item_input, labels = [],[],[]\n",
    "#     num_users = train.shape[0]\n",
    "#     for (u, i) in train.keys():\n",
    "#         # positive instance\n",
    "#         user_input.append(u)\n",
    "#         item_input.append(i)\n",
    "#         labels.append(1)\n",
    "#         # negative instances\n",
    "#         for t in xrange(num_negatives):\n",
    "#             j = np.random.randint(num_items)\n",
    "#             while train.has_key((u, j)):\n",
    "#                 j = np.random.randint(num_items)\n",
    "#             user_input.append(u)\n",
    "#             item_input.append(j)\n",
    "#             labels.append(0)\n",
    "#     return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "models_and_input = [('GMF', gmf_model, mrr_GMF, ndcg_GMF)]\n",
    "\n",
    "for model_and_input in models_and_input:\n",
    "    model_name, model, mrr, ndcg = model_and_input\n",
    "    best_mrr, best_ndcg, best_iter = mrr, ndcg, -1\n",
    "    print(f\"Running on Model: {model_name}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        t1 = time()\n",
    "        \n",
    "        # Generate training instances\n",
    "        user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "\n",
    "        # Training\n",
    "        print('Training - start')\n",
    "        hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                         np.array(labels), # labels \n",
    "                         \n",
    "                         batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
    "        print('Training - end')\n",
    "        \n",
    "        t2 = time()\n",
    "\n",
    "        # Evaluation\n",
    "        print('Evaluation - start') \n",
    "\n",
    "        if epoch % verbose == 0:\n",
    "            mrr, ndcg = evaluate_model(model, test_rating, test_negative)\n",
    "            loss = hist.history['loss'][0]\n",
    "            print(f'Iteration {epoch} [{t2-t1:.1f} s]: MRR = {mrr:.4f}, NDCG = {ndcg:.4f}, loss = {loss:.4f} [{time()-t2:.1f} s]')\n",
    "\n",
    "            if mrr > best_mrr:\n",
    "                best_mrr, best_ndcg, best_iter = mrr, ndcg, epoch\n",
    "        print(f\"---------End Epoch: {epoch}---------\")\n",
    "\n",
    "    print(f\"End. Best Iteration {best_iter}:  MRR = {best_mrr:.4f}, NDCG = {best_ndcg:.4f}. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "- Compare between different heuristics of item price to weights mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "- Present the comparison results, discuss the results and the trade-offs and optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "- Compare between different heuristics of item price to weights mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "- Verify and present that the learning is ‘healthy’ (no overfitting, no under-fitting and that the results make sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 3: Loss function\n",
    "Cold start or users\\items with a small number of interactions is a very common scenario in real world. In this question you will plan how you can leverage content based features to handle the cold start scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) Take a look at the original MovieLens 1M dataset. Which user and movie features could you use to enhance your recommender system and provide effective recommendations to users or items with a small number of ratings. (5 points)\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) Describe a neural network based model to incorporate user or movie related features to the recommender system. Explain your suggestion.<br>\n",
    "(There is no need to implement, provide pseudo code\\visual) (5 points)\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) How will you incorporate movie genres into the recommender system? How will you handle movies which belong to multiple genres? Explain the challenge and the proposed solution <br>\n",
    "(There is no need to implement, provide pseudo code\\visual) (5 points)\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
