{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations Systems\n",
    "## Course Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to let you practice in a data scientist daily work by leveraging recommender\n",
    "systems algorithms you learnt in the course and customize them in order to solve real business\n",
    "problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset based on the <a href='https://grouplens.org/datasets/movielens/1m/'>MovieLens 1M rating dataset</a> after some pre-processing to adapt it to an implicit feedback use case scenario.  \n",
    "You can download the dataset used by <a href='https://github.com/hexiangnan/neural_collaborative_filtering'>this implementation</a> of the paper Neural Collaborative Filtering or from the NeuralCollaborativeFiltering_implicit notebook in Moodle.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# from keras.layers import Embedding, Input, Dense, Reshape,  Flatten, Dropout\n",
    "# from keras.regularizers import l2\n",
    "# from keras import backend as K\n",
    "# from keras import initializers\n",
    "# from keras.initializers import RandomNormal\n",
    "# from keras.models import Sequential, Model, load_model, save_model\n",
    "# from keras.layers.core import Dense, Lambda, Activation\n",
    "# from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "# from keras.layers import Multiply, Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "\n",
    "# Read the training file\n",
    "training = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.train.rating', sep='\\t', names=column_names)\n",
    "\n",
    "# Read the test file\n",
    "test_rating = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.test.rating', sep='\\t', names=column_names)\n",
    "\n",
    "\n",
    "negative_ids = ['(user_id, item_id)']\n",
    "\n",
    "for i in range(1,100):\n",
    "    negative_ids.append(f'id-{i}')\n",
    "\n",
    "test_negative = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.test.negative', sep='\\t', names=negative_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Matrix Factorization with custom loss (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) Answer:\n",
    "\n",
    "Given\n",
    "$$\n",
    "L=-\\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(y_{i, j} \\log \\left(\\sigma\\left(\\mu+p_{i}+o_{j}+\\boldsymbol{u}_{i}^{T} v_{j}\\right)\\right)+\\left(1-y_{i, j}\\right) \\log \\left(1-\\sigma\\left(\\mu+p_{i}+o_{j}+\\boldsymbol{u}_{i}^{T} v_{j}\\right)\\right)\\right)\n",
    "$$\n",
    "\n",
    "Note that:\n",
    "$$\n",
    "\\log (\\sigma (z)) \\newline\n",
    "= \\log (\\frac{1}{1+e^{-z}}) \\newline\n",
    "= \\log(1) - \\log(1+e^{-z}) \\newline\n",
    "= -\\log(1+e^{-z})\n",
    "$$\n",
    "* (Here, we used the fact that $\\log(\\frac{x}{y}) = \\log(x) - \\log(y)$)\n",
    "\n",
    "Similarly:\n",
    "$$\n",
    "\\log (1 - \\sigma (z)) \\newline \\newline\n",
    "= \\log (1 - \\frac{1}{1+e^{-z}}) \\newline\n",
    "= \\log (\\frac{1+e^{-z} - 1}{1+e^{-z}}) \\newline\n",
    "= \\log (\\frac{e^{-z}}{1+e^{-z}}) \\newline\n",
    "= \\log(e^{-z}) - \\log(1+e^{-z}) \\newline\n",
    "= -z - \\log(1+e^{-z}) \\newline\n",
    "= -\\big( z + \\log(1+e^{-z}) \\big)\n",
    "$$\n",
    "\n",
    "\n",
    "**Denote** $\\boldsymbol{z = \\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j}}$ <br>\n",
    "Now, using $z$ and plugging in the two simplified expressions from above, we obtain:\n",
    "$$\n",
    "L=-\\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(y_{i, j} \\log \\left(\\sigma\\left(z\\right)\\right)+\\left(1-y_{i, j}\\right) \\log \\left(1-\\sigma\\left(z\\right)\\right)\\right) \\newline\n",
    "=-\\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(-1 \\cdot y_{i, j} \\log(1+e^{-z}) +\\left(1-y_{i, j}\\right) \n",
    "\\left( -(z + \\log(1+e^{-z}))\\right)\\right) \\newline\n",
    "=-\\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(-1 \\cdot y_{i, j} \\log(1+e^{-z}) - \\left(1-y_{i, j}\\right)(z + \\log(1+e^{-z})) \\right) \\newline\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(y_{i, j} \\log(1+e^{-z}) + \\left(1-y_{i, j}\\right)(z + \\log(1+e^{-z})) \\right) \\newline\n",
    "$$\n",
    "\n",
    "After opening up the multiplications, we end up with:\n",
    "$$\n",
    "L= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left( z - y_{i, j} z + \\log(1+e^{-z}) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "- Reference: https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need now is to compute the partial derivatives of $L$ w.r.t the user and item latent vector weights ($u_i,  v_j$), the global bias ($\\mu$), user bias ($p_i$) and item bias ($o_j$) variables for user i and item j.<br>\n",
    "\n",
    "Each parameter will use the partial derivative of L w.r.t the parameter we're differentiating according to:<br>\n",
    "Meaning, these are the update rules:\n",
    "$$\n",
    "u = u - \\frac{\\partial L}{\\partial u} \\newline\n",
    "v = v - \\frac{\\partial L}{\\partial v} \\newline\n",
    "\\mu = \\mu - \\frac{\\partial L}{\\partial \\mu} \\newline\n",
    "p = p - \\frac{\\partial L}{\\partial p} \\newline\n",
    "o = o - \\frac{\\partial L}{\\partial o}\n",
    "$$\n",
    "\n",
    "Using the \"Chain Rule\" - we solve each derivative separately and then plug back in:\n",
    "\n",
    "\n",
    "- **user latent vector weights ($u_i$):**\n",
    "$$\n",
    "\\frac{\\partial [\\alpha_{j} z]}{\\partial u_i}\n",
    "= \\frac{\\partial [\\alpha_{j} (\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j})]}{\\partial u_i} \n",
    "= \\alpha_{j} v_{j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial [-\\alpha_{j}  y_{i, j} z]}{\\partial u_i}\n",
    "= \\frac{\\partial [- \\alpha_{j} y_{i, j}(\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j})]} {\\partial u_i} \n",
    "= -\\alpha_{j} y_{i, j} v_{j}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial [\\alpha_{j}  \\log(1+e^{-z})]}{\\partial u_i}\n",
    "= \\frac{\\alpha_j e^{-z} \\frac{\\partial z}{\\partial u_i}} {1 + e^{-z}}\n",
    "= - \\frac{\\alpha_{j} e^{-z} v_{j}}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "* (Last transition, log derivative, follows from [here](https://www.symbolab.com/solver/step-by-step/%5Cleft(log_%7Be%7D%5Cleft(1%2Be%5E%7B-2x%7D%5Cright)%5Cright)%5E%7B'%7D))\n",
    "\n",
    "Hence,\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial u_i}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\big( \\alpha_{j} v_{j} - \\alpha_{j} y_{i, j} v_{j} - \\frac{\\alpha_{j} e^{-z} v_{j}}{1 + e^{-z}} \\big) \n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} v_{j} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$\n",
    "\n",
    "\n",
    "- **item latent vector weights ($v_j$)** is very similar, since both terms are multiplied with each other:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial v_j}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\big( \\alpha_{j} {u}_{i}^{T} - \\alpha_{j} y_{i, j} {u}_{i}^{T} - \\frac{\\alpha_{j} e^{-z} {u}_{i}^{T}}{1 + e^{-z}} \\big) \n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} {u}_{i}^{T} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$\n",
    "\n",
    "\n",
    "- **the global bias, $\\mu$:**\n",
    "$$\n",
    "\\frac{\\partial [\\alpha_{j} z]}{\\partial \\mu}\n",
    "= \\frac{\\partial [\\alpha_{j} (\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j})]}{\\partial \\mu} \n",
    "= \\alpha_{j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial [-\\alpha_{j}  y_{i, j} z]}{\\partial \\mu}\n",
    "= \\frac{\\partial [- \\alpha_{j} y_{i, j}(\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j})]} {\\partial \\mu} \n",
    "= -\\alpha_{j} y_{i, j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial [\\alpha_{j}  \\log(1+e^{-z})]}{\\partial \\mu}\n",
    "= \\frac{\\partial [\\alpha_{j} \\log(1 + e^{\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j}})]} {\\partial \\mu} \n",
    "= \\frac{\\alpha_j e^{-z} \\frac{\\partial z}{\\partial u_i}} {1 + e^{-z}}\n",
    "= - \\frac{\\alpha_{j} e^{-z}}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mu}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\big( \\alpha_{j} - \\alpha_{j} y_{i, j} - \\frac{\\alpha_{j} e^{-z}}{1 + e^{-z}} \\big) \n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$\n",
    "\n",
    "\n",
    "- **user bias $p_i$** follows the same calculation as $\\mu$ since it's added as a standalone addition:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial p_i}\n",
    "= \\frac{\\partial L}{\\partial \\mu}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$\n",
    "\n",
    "\n",
    "- **item bias $o_j$** follows the same calculation as $\\mu$ since it's added as a standalone addition:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial o_j}\n",
    "= \\frac{\\partial L}{\\partial \\mu}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) Answer:\n",
    "\n",
    "Given the prices of the different items from the catalog, we can set the weights for each training instance based on the **item price divided by the maximal price** we have.\n",
    "Meaning, we're giving each weight its relative \"significance\".\n",
    "\n",
    "This is very intuitive and straightforward - The more expensive an item is, the larger weight it's given, where weights' values range from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 2: Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A) Use the item_price.csv file to get the prices of each item. Explore the price distribution of items.\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3701</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3702</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3703</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3705</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3706 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      price\n",
       "item       \n",
       "0         4\n",
       "1         1\n",
       "2         1\n",
       "3         2\n",
       "4         2\n",
       "...     ...\n",
       "3701     25\n",
       "3702      4\n",
       "3703      1\n",
       "3704      4\n",
       "3705      4\n",
       "\n",
       "[3706 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_price = pd.read_csv('./item_price.csv')\n",
    "item_price.set_index('item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_items = item_price.shape[0]\n",
    "prices = item_price['price']\n",
    "min_price = prices.min()\n",
    "max_price = prices.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x205edcd7c10>,\n",
       "  <matplotlib.axis.XTick at 0x205edcd7be0>,\n",
       "  <matplotlib.axis.XTick at 0x205edcd72b0>,\n",
       "  <matplotlib.axis.XTick at 0x205edd884c0>,\n",
       "  <matplotlib.axis.XTick at 0x205edd889d0>],\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(item_price.price, align='mid', bins=50)\n",
    "plt.xticks(item_price.price.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price\n",
       "1     1326\n",
       "2      896\n",
       "4      709\n",
       "9      589\n",
       "25     186\n",
       "Name: item, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_price.groupby('price')['item'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3706.000000</td>\n",
       "      <td>3706.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1852.500000</td>\n",
       "      <td>4.291689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1069.974377</td>\n",
       "      <td>5.496992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>926.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1852.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2778.750000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              item        price\n",
       "count  3706.000000  3706.000000\n",
       "mean   1852.500000     4.291689\n",
       "std    1069.974377     5.496992\n",
       "min       0.000000     1.000000\n",
       "25%     926.250000     1.000000\n",
       "50%    1852.500000     2.000000\n",
       "75%    2778.750000     4.000000\n",
       "max    3705.000000    25.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3706 items with prices distributed from [1, 2, 4, 9, 25].<br>\n",
    "From the histogram and the data we see that most of the items are priced 2 and lower - ((1: 1326), (2: 896), (4: 709), (9: 589), (25: 186))<br>\n",
    "It doesn't look like we can deduce a behaviour of a specific distribution (like normal/uniform etc.), especially since we don't have enough data other than ID numbers and prices...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B) To evaluate the performance of the price sensitive model we add another metric Revenue@K which measures the overall revenue from the top 5 recommended hits.\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "def revenue_at_K(df, column_name, K):\n",
    "    ''' For each user: sum the prices of the top K recommended items which were rated as the revenue from the user '''\n",
    "    result = df.nlargest(K, column_name)\n",
    "    result_sum = result.sum()\n",
    "    return result_sum[column_name]\n",
    "\n",
    "\n",
    "# Calculate the mean revenue from all users\n",
    "\n",
    "print(revenue_at_K(item_price, 'price', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C) Suggest a metric of your own which will incorporate both the ranking of the recommended items as well as its price. Explain why this metric is suitable and demonstrate it as part of the evaluation in point e below.\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D) Select one of the models presented in the Neural Collaborative Filtering paper and incorporate the movie price to the loss function as part of training\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GMF_model(num_users, num_items, latent_dim, regs=None, activation='sigmoid'):\n",
    "    '''Generalized Matrix Factorization'''\n",
    "\n",
    "    if not regs:\n",
    "        regs = [[0,0]]\n",
    "    \n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01))  #init = init_normal, \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings \n",
    "    predict_vector = Multiply()([user_latent, item_latent]) #merge([user_latent, item_latent], mode = 'mul')\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation=activation, kernel_initializer='lecun_uniform', name = 'prediction')(predict_vector)\n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "M7nlDtDaCakQ"
   },
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "\n",
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [0]*((num_negatives + 1)*len(train)),[0]*((num_negatives + 1)*len(train)),[1]\n",
    "    num_users = train.shape[0]\n",
    "    all_items = training.item_id.unique().argsort()\n",
    "    \n",
    "    negatives = [0]*num_negatives\n",
    "    labels.extend(negatives)\n",
    "    total_labels = []\n",
    "    list(map(lambda x: total_labels.extend(labels), range(len(train))))\n",
    "#     return\n",
    "    percent_1 = int(len(train)/100)\n",
    "    ic(percent_1)\n",
    "    \n",
    "    items_the_user_didnt_rank = None\n",
    "    prev_user = -1\n",
    "    chosen_item_per_user = []\n",
    "    for idx_i in range(len(train)):\n",
    "        curr_index = idx_i * (num_negatives + 1)\n",
    "        if idx_i != 0 and idx_i % percent_1 == 0:\n",
    "            print(f'{int(idx_i/percent_1)}%')\n",
    "        u = train.iloc[idx_i].user_id\n",
    "        i = train.iloc[idx_i].item_id\n",
    "\n",
    "        user_input[curr_index:curr_index + (num_negatives + 1)] = [u]*(num_negatives + 1)\n",
    "\n",
    "        item_input[curr_index] = i\n",
    "        \n",
    "        if u != prev_user:\n",
    "            items = training[training['user_id'] == u].item_id.to_numpy().argsort()\n",
    "            items_the_user_didnt_rank = all_items[~np.in1d(all_items,items)]\n",
    "            prev_user = u\n",
    "#             chosen_item_per_user = []\n",
    "\n",
    "#         items_the_user_didnt_rank = items_the_user_didnt_rank[~np.in1d(items_the_user_didnt_rank,chosen_item_per_user)]\n",
    "        sample_items = items_the_user_didnt_rank[np.random.choice(len(items_the_user_didnt_rank), size=num_negatives, replace=False)]\n",
    "        item_input[curr_index+1:curr_index + (num_negatives + 1)] = sample_items\n",
    "#         chosen_item_per_user.extend(sample_items)\n",
    "\n",
    "    return user_input, item_input, total_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### E) \n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 3: Loss function\n",
    "Cold start or users\\items with a small number of interactions is a very common scenario in real world. In this question you will plan how you can leverage content based features to handle the cold start scenario.\n",
    "\n",
    "\n",
    "A) Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
