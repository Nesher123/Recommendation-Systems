{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations Systems\n",
    "## Course Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to let you practice in a data scientist daily work by leveraging recommender\n",
    "systems algorithms you learnt in the course and customize them in order to solve real business\n",
    "problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset based on the <a href='https://grouplens.org/datasets/movielens/1m/'>MovieLens 1M rating dataset</a> after some pre-processing to adapt it to an implicit feedback use case scenario.  \n",
    "You can download the dataset used by <a href='https://github.com/hexiangnan/neural_collaborative_filtering'>this implementation</a> of the paper Neural Collaborative Filtering or from the NeuralCollaborativeFiltering_implicit notebook in Moodle.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d314f57b62da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0ml2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.layers import Embedding, Input, Dense, Reshape,  Flatten, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras.layers import Multiply, Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "\n",
    "# Read the training file\n",
    "training = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.train.rating', sep='\\t', names=column_names)\n",
    "\n",
    "# Read the test file\n",
    "test_rating = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.test.rating', sep='\\t', names=column_names)\n",
    "\n",
    "\n",
    "negative_ids = ['(user_id, item_id)']\n",
    "\n",
    "for i in range(1,100):\n",
    "    negative_ids.append(f'id-{i}')\n",
    "\n",
    "test_negative = pd.read_csv('./neural_collaborative_filtering/Data/ml-1m.test.negative', sep='\\t', names=negative_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Matrix Factorization with custom loss (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Given\n",
    "$$\n",
    "L=-\\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(y_{i, j} \\log \\left(\\sigma\\left(\\mu+p_{i}+o_{j}+\\boldsymbol{u}_{i}^{T} v_{j}\\right)\\right)+\\left(1-y_{i, j}\\right) \\log \\left(1-\\sigma\\left(\\mu+p_{i}+o_{j}+\\boldsymbol{u}_{i}^{T} v_{j}\\right)\\right)\\right)\n",
    "$$\n",
    "\n",
    "Note that:\n",
    "$$\n",
    "\\log (\\sigma (z)) \\newline\n",
    "= \\log (\\frac{1}{1+e^{-z}}) \\newline\n",
    "= \\log(1) - \\log(1+e^{-z}) \\newline\n",
    "= -\\log(1+e^{-z})\n",
    "$$\n",
    "* (Here, we used the fact that $\\log(\\frac{x}{y}) = \\log(x) - \\log(y)$)\n",
    "\n",
    "Similarly:\n",
    "$$\n",
    "\\log (1 - \\sigma (z)) \\newline \\newline\n",
    "= \\log (1 - \\frac{1}{1+e^{-z}}) \\newline\n",
    "= \\log (\\frac{1+e^{-z} - 1}{1+e^{-z}}) \\newline\n",
    "= \\log (\\frac{e^{-z}}{1+e^{-z}}) \\newline\n",
    "= \\log(e^{-z}) - \\log(1+e^{-z}) \\newline\n",
    "= -z - \\log(1+e^{-z}) \\newline\n",
    "= -\\big( z + \\log(1+e^{-z}) \\big)\n",
    "$$\n",
    "\n",
    "\n",
    "**Denote** $\\boldsymbol{z = \\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j}}$ <br>\n",
    "Now, using $z$ and plugging in the two simplified expressions from above, we obtain:\n",
    "$$\n",
    "L=-\\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(y_{i, j} \\log \\left(\\sigma\\left(z\\right)\\right)+\\left(1-y_{i, j}\\right) \\log \\left(1-\\sigma\\left(z\\right)\\right)\\right) \\newline\n",
    "=-\\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(-1 \\cdot y_{i, j} \\log(1+e^{-z}) +\\left(1-y_{i, j}\\right) \n",
    "\\left( -(z + \\log(1+e^{-z}))\\right)\\right) \\newline\n",
    "=-\\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(-1 \\cdot y_{i, j} \\log(1+e^{-z}) - \\left(1-y_{i, j}\\right)(z + \\log(1+e^{-z})) \\right) \\newline\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left(y_{i, j} \\log(1+e^{-z}) + \\left(1-y_{i, j}\\right)(z + \\log(1+e^{-z})) \\right) \\newline\n",
    "$$\n",
    "\n",
    "After opening up the multiplications, we end up with:\n",
    "$$\n",
    "L= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j}\\left( z - y_{i, j} z + \\log(1+e^{-z}) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "- Reference: https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need now is to compute the partial derivatives of $L$ w.r.t the user and item latent vector weights ($u_i,  v_j$), the global bias ($\\mu$), user bias ($p_i$) and item bias ($o_j$) variables for user i and item j.<br>\n",
    "\n",
    "Using the \"Chain Rule\" - we solve each derivative separately and then plug back in:\n",
    "\n",
    "\n",
    "- **user latent vector weights ($u_i$):**\n",
    "$$\n",
    "\\frac{\\partial [\\alpha_{j} z]}{\\partial u_i}\n",
    "= \\frac{\\partial [\\alpha_{j} (\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j})]}{\\partial u_i} \n",
    "= \\alpha_{j} v_{j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial [-\\alpha_{j}  y_{i, j} z]}{\\partial u_i}\n",
    "= \\frac{\\partial [- \\alpha_{j} y_{i, j}(\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j})]} {\\partial u_i} \n",
    "= -\\alpha_{j} y_{i, j} v_{j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial [\\alpha_{j}  \\log(1+e^{-z})]}{\\partial u_i}\n",
    "= \\frac{\\partial [\\alpha_{j} \\log(1 + e^{\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j}})]} {\\partial u_i} \n",
    "= \\frac{\\alpha_j e^{-z} \\frac{\\partial z}{\\partial u_i}} {1 + e^{-z}}\n",
    "= - \\frac{\\alpha_{j} e^{-z} v_{j}}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial u_i}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\big( \\alpha_{j} v_{j} - \\alpha_{j} y_{i, j} v_{j} - \\frac{\\alpha_{j} e^{-z} v_{j}}{1 + e^{-z}} \\big) \n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} v_{j} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$\n",
    "\n",
    "\n",
    "- **item latent vector weights ($v_j$)** is very similar, since both terms are multiplied with each other:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial v_j}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\big( \\alpha_{j} {u}_{i}^{T} - \\alpha_{j} y_{i, j} {u}_{i}^{T} - \\frac{\\alpha_{j} e^{-z} {u}_{i}^{T}}{1 + e^{-z}} \\big) \n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} {u}_{i}^{T} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$\n",
    "\n",
    "\n",
    "- **the global bias, $\\mu$:**\n",
    "$$\n",
    "\\frac{\\partial [\\alpha_{j} z]}{\\partial \\mu}\n",
    "= \\frac{\\partial [\\alpha_{j} (\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j})]}{\\partial \\mu} \n",
    "= \\alpha_{j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial [-\\alpha_{j}  y_{i, j} z]}{\\partial \\mu}\n",
    "= \\frac{\\partial [- \\alpha_{j} y_{i, j}(\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j})]} {\\partial \\mu} \n",
    "= -\\alpha_{j} y_{i, j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial [\\alpha_{j}  \\log(1+e^{-z})]}{\\partial \\mu}\n",
    "= \\frac{\\partial [\\alpha_{j} \\log(1 + e^{\\mu+p_{i}+o_{j}+{u}_{i}^{T} v_{j}})]} {\\partial \\mu} \n",
    "= \\frac{\\alpha_j e^{-z} \\frac{\\partial z}{\\partial u_i}} {1 + e^{-z}}\n",
    "= - \\frac{\\alpha_{j} e^{-z}}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mu}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\big( \\alpha_{j} - \\alpha_{j} y_{i, j} - \\frac{\\alpha_{j} e^{-z}}{1 + e^{-z}} \\big) \n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$\n",
    "\n",
    "\n",
    "- **user bias $p_i$** follows the same calculation as $\\mu$ since it's added as a standalone addition:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial p_i}\n",
    "= \\frac{\\partial L}{\\partial \\mu}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$\n",
    "\n",
    "\n",
    "- **item bias $o_j$** follows the same calculation as $\\mu$ since it's added as a standalone addition:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial o_j}\n",
    "= \\frac{\\partial L}{\\partial \\mu}\n",
    "= \\frac{1}{N} \\sum_{(i, j) \\in S}^{N} \\alpha_{j} \\big( 1 - y_{i, j} - \\frac{e^{-z}}{1 + e^{-z}} \\big) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 2: Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A) Use the item_price.csv file to get the prices of each item. Explore the price distribution of items.\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_price = pd.read_csv('./item_price.csv')\n",
    "item_price.set_index('item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_items = item_price.shape[0]\n",
    "prices = item_price['price']\n",
    "min_price = prices.min()\n",
    "max_price = prices.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(item_price.item, weights=item_price.price, bins=number_of_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_price.groupby('price')['item'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3706 items with prices distributed from [1, 2, 4, 9, 25].<br>\n",
    "From the histogram and the data we see that most of the items are priced 2 and lower - ((1: 1326), (2: 896), (4: 709), (9: 589), (25: 186))<br>\n",
    "It doesn't look like we can deduce a behaviour of a specific distribution (like normal/uniform etc.), especially since we don't have enough data other than ID numbers and prices...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B) To evaluate the performance of the price sensitive model we add another metric Revenue@K which measures the overall revenue from the top 5 recommended hits.\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revenue_at_K(df, column_name, K):\n",
    "    ''' For each user: sum the prices of the top K recommended items which were rated as the revenue from the user '''\n",
    "    result = df.nlargest(K, column_name)\n",
    "    result_sum = result.sum()\n",
    "    return result_sum[column_name]\n",
    "\n",
    "\n",
    "# Calculate the mean revenue from all users\n",
    "\n",
    "print(revenue_at_K(item_price, 'price', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C) Suggest a metric of your own which will incorporate both the ranking of the recommended items as well as its price. Explain why this metric is suitable and demonstrate it as part of the evaluation in point e below.\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D) Select one of the models presented in the Neural Collaborative Filtering paper and incorporate the movie price to the loss function as part of training\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GMF_model(num_users, num_items, latent_dim, regs=None, activation='sigmoid'):\n",
    "    '''Generalized Matrix Factorization'''\n",
    "\n",
    "    if not regs:\n",
    "        regs = [[0,0]]\n",
    "    \n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][0]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01)) #init = init_normal,\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
    "                                   embeddings_regularizer = l2(regs[0][1]), input_length=1,embeddings_initializer=RandomNormal(mean=0.0, stddev=0.01))  #init = init_normal, \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings \n",
    "    predict_vector = Multiply()([user_latent, item_latent]) #merge([user_latent, item_latent], mode = 'mul')\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation=activation, kernel_initializer='lecun_uniform', name = 'prediction')(predict_vector)\n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### E) \n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Question 3: Loss function\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
